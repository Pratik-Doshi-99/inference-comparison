INFO 01-23 06:45:59 engine.py:267] Added request cmpl-5a73f8bd5e1942859f770f7ce185ae03-0.
INFO 01-23 06:46:02 metrics.py:467] Avg prompt throughput: 1.6 tokens/s, Avg generation throughput: 67.7 tokens/s, Running: 10 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.
INFO 01-23 06:46:03 logger.py:37] Received request cmpl-1ccd6a2700c8482a8d4eda7b1f43ddaf-0: prompt: 'Explain quantum computing in simple terms', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 31228, 25213, 304, 4382, 3878], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46158 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:46:03 engine.py:267] Added request cmpl-1ccd6a2700c8482a8d4eda7b1f43ddaf-0.
INFO 01-23 06:46:07 metrics.py:467] Avg prompt throughput: 1.6 tokens/s, Avg generation throughput: 75.8 tokens/s, Running: 11 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.4%, CPU KV cache usage: 0.0%.
INFO 01-23 06:46:07 logger.py:37] Received request cmpl-bb441f5af2f4456aa7eab74a8ae3ff01-0: prompt: 'Explain quantum computing in simple terms', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 31228, 25213, 304, 4382, 3878], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44214 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:46:08 engine.py:267] Added request cmpl-bb441f5af2f4456aa7eab74a8ae3ff01-0.
INFO 01-23 06:46:08 logger.py:37] Received request cmpl-171e98fad10d4b6090f6ebc18e624f33-0: prompt: 'Write a poem about artificial intelligence', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 33894, 922, 21075, 11478], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55476 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:46:08 engine.py:267] Added request cmpl-171e98fad10d4b6090f6ebc18e624f33-0.
INFO 01-23 06:46:09 logger.py:37] Received request cmpl-8f427781ab9a42428d3ae2f46affef61-0: prompt: 'Write a poem about artificial intelligence', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 33894, 922, 21075, 11478], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55488 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:46:10 engine.py:267] Added request cmpl-8f427781ab9a42428d3ae2f46affef61-0.
INFO 01-23 06:46:10 logger.py:37] Received request cmpl-18103a3945854b6db7ffb89f8d4b9fce-0: prompt: 'Write a poem about artificial intelligence', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 33894, 922, 21075, 11478], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55498 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:46:10 engine.py:267] Added request cmpl-18103a3945854b6db7ffb89f8d4b9fce-0.
INFO 01-23 06:46:12 logger.py:37] Received request cmpl-ca8dab9c8dd84015934ac78195aceb43-0: prompt: 'Write a poem about artificial intelligence', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 33894, 922, 21075, 11478], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55506 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:46:12 engine.py:267] Added request cmpl-ca8dab9c8dd84015934ac78195aceb43-0.
INFO 01-23 06:46:12 metrics.py:467] Avg prompt throughput: 5.8 tokens/s, Avg generation throughput: 81.6 tokens/s, Running: 12 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.1%, CPU KV cache usage: 0.0%.
INFO 01-23 06:46:12 logger.py:37] Received request cmpl-11d2859c85744c0cae69cfcb52a944ce-0: prompt: 'Explain quantum computing in simple terms', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 31228, 25213, 304, 4382, 3878], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44222 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:46:12 engine.py:267] Added request cmpl-11d2859c85744c0cae69cfcb52a944ce-0.
INFO 01-23 06:46:13 logger.py:37] Received request cmpl-b86d61ed12fd4e139fa0c046836ac28c-0: prompt: 'Write a poem about artificial intelligence', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 33894, 922, 21075, 11478], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57724 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:46:13 engine.py:267] Added request cmpl-b86d61ed12fd4e139fa0c046836ac28c-0.
INFO 01-23 06:46:15 logger.py:37] Received request cmpl-e4f0b4ed8ad246999a3f83f075e62175-0: prompt: 'Write a poem about artificial intelligence', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 33894, 922, 21075, 11478], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57736 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:46:15 engine.py:267] Added request cmpl-e4f0b4ed8ad246999a3f83f075e62175-0.
INFO 01-23 06:46:17 metrics.py:467] Avg prompt throughput: 5.8 tokens/s, Avg generation throughput: 91.6 tokens/s, Running: 13 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.2%, CPU KV cache usage: 0.0%.
INFO 01-23 06:46:17 logger.py:37] Received request cmpl-ba0028ad3c1143dbbfe87f98db8b8466-0: prompt: 'Explain quantum computing in simple terms', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 31228, 25213, 304, 4382, 3878], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60722 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:46:18 engine.py:267] Added request cmpl-ba0028ad3c1143dbbfe87f98db8b8466-0.
INFO 01-23 06:46:18 logger.py:37] Received request cmpl-1bc1aca20e4c43d1a07122282ef85ac6-0: prompt: 'Write a poem about artificial intelligence', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 33894, 922, 21075, 11478], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57748 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:46:18 engine.py:267] Added request cmpl-1bc1aca20e4c43d1a07122282ef85ac6-0.
INFO 01-23 06:46:21 logger.py:37] Received request cmpl-e23a58077a1a4b92a75414be02dcb5aa-0: prompt: 'Write a poem about artificial intelligence', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 33894, 922, 21075, 11478], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57752 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:46:21 engine.py:267] Added request cmpl-e23a58077a1a4b92a75414be02dcb5aa-0.
INFO 01-23 06:46:22 metrics.py:467] Avg prompt throughput: 4.3 tokens/s, Avg generation throughput: 97.1 tokens/s, Running: 14 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.
INFO 01-23 06:46:23 logger.py:37] Received request cmpl-26a6455df29e4ac68002a5df39af4455-0: prompt: 'Explain quantum computing in simple terms', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 31228, 25213, 304, 4382, 3878], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60724 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:46:23 engine.py:267] Added request cmpl-26a6455df29e4ac68002a5df39af4455-0.
INFO 01-23 06:46:24 logger.py:37] Received request cmpl-91b5a82df04946ada737e324248829eb-0: prompt: 'Write a poem about artificial intelligence', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 33894, 922, 21075, 11478], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46142 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:46:24 engine.py:267] Added request cmpl-91b5a82df04946ada737e324248829eb-0.
INFO 01-23 06:46:27 metrics.py:467] Avg prompt throughput: 2.9 tokens/s, Avg generation throughput: 103.6 tokens/s, Running: 15 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 01-23 06:46:27 logger.py:37] Received request cmpl-8934ca9c7660459188c0dff270a4a326-0: prompt: 'Write a poem about artificial intelligence', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 33894, 922, 21075, 11478], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46148 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:46:28 engine.py:267] Added request cmpl-8934ca9c7660459188c0dff270a4a326-0.
INFO 01-23 06:46:29 logger.py:37] Received request cmpl-218ad2fffe484104ab1128eab66a45e4-0: prompt: 'Explain quantum computing in simple terms', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 31228, 25213, 304, 4382, 3878], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39008 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:46:29 engine.py:267] Added request cmpl-218ad2fffe484104ab1128eab66a45e4-0.
INFO 01-23 06:46:32 logger.py:37] Received request cmpl-b3c619141a4147cca380d73e074e8a73-0: prompt: 'Write a poem about artificial intelligence', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 33894, 922, 21075, 11478], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46158 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:46:32 engine.py:267] Added request cmpl-b3c619141a4147cca380d73e074e8a73-0.
INFO 01-23 06:46:32 metrics.py:467] Avg prompt throughput: 4.3 tokens/s, Avg generation throughput: 109.2 tokens/s, Running: 16 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.7%, CPU KV cache usage: 0.0%.
INFO 01-23 06:46:35 logger.py:37] Received request cmpl-992f3582ff4c4ac2b279d0ea369d5a8d-0: prompt: 'Explain quantum computing in simple terms', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 31228, 25213, 304, 4382, 3878], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56690 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:46:36 engine.py:267] Added request cmpl-992f3582ff4c4ac2b279d0ea369d5a8d-0.
INFO 01-23 06:46:36 logger.py:37] Received request cmpl-58a46dd6e71e4e39a26cf6445a73cbe5-0: prompt: 'Write a poem about artificial intelligence', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 33894, 922, 21075, 11478], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44214 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:46:36 engine.py:267] Added request cmpl-58a46dd6e71e4e39a26cf6445a73cbe5-0.
INFO 01-23 06:46:36 logger.py:37] Received request cmpl-a90c4ce76a934c06b9ae9d62f58db8f6-0: prompt: 'How does photosynthesis work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 4438, 1587, 7397, 74767, 990, 30], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55476 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:46:36 engine.py:267] Added request cmpl-a90c4ce76a934c06b9ae9d62f58db8f6-0.
INFO 01-23 06:46:37 metrics.py:467] Avg prompt throughput: 4.3 tokens/s, Avg generation throughput: 114.2 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.9%, CPU KV cache usage: 0.0%.
INFO 01-23 06:46:38 logger.py:37] Received request cmpl-6e8df1c1b61349aeac57313113c70dcf-0: prompt: 'How does photosynthesis work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 4438, 1587, 7397, 74767, 990, 30], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55488 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:46:38 engine.py:267] Added request cmpl-6e8df1c1b61349aeac57313113c70dcf-0.
INFO 01-23 06:46:39 logger.py:37] Received request cmpl-966da04d94a3475a9e0e47dfdea6f62c-0: prompt: 'How does photosynthesis work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 4438, 1587, 7397, 74767, 990, 30], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55498 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:46:39 engine.py:267] Added request cmpl-966da04d94a3475a9e0e47dfdea6f62c-0.
INFO 01-23 06:46:40 logger.py:37] Received request cmpl-6fd546f2adae4a6f83ae4dca669b30f2-0: prompt: 'How does photosynthesis work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 4438, 1587, 7397, 74767, 990, 30], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55506 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:46:40 engine.py:267] Added request cmpl-6fd546f2adae4a6f83ae4dca669b30f2-0.
INFO 01-23 06:46:41 logger.py:37] Received request cmpl-2fd1d604667e4612a4f28b363ca7c442-0: prompt: 'Write a poem about artificial intelligence', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 33894, 922, 21075, 11478], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44222 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:46:41 engine.py:267] Added request cmpl-2fd1d604667e4612a4f28b363ca7c442-0.
INFO 01-23 06:46:42 logger.py:37] Received request cmpl-00b735976c4d4d258e58b9afdbe35866-0: prompt: 'How does photosynthesis work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 4438, 1587, 7397, 74767, 990, 30], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57724 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:46:42 engine.py:267] Added request cmpl-00b735976c4d4d258e58b9afdbe35866-0.
INFO 01-23 06:46:42 metrics.py:467] Avg prompt throughput: 7.0 tokens/s, Avg generation throughput: 117.7 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 01-23 06:46:42 logger.py:37] Received request cmpl-e2da1c9ebe934534b34402eed2b80a24-0: prompt: 'Explain quantum computing in simple terms', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 31228, 25213, 304, 4382, 3878], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56696 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:46:42 engine.py:267] Added request cmpl-e2da1c9ebe934534b34402eed2b80a24-0.
INFO 01-23 06:46:44 logger.py:37] Received request cmpl-bb883940c6da482d826aadf7c11d20cb-0: prompt: 'How does photosynthesis work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 4438, 1587, 7397, 74767, 990, 30], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57736 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:46:44 engine.py:267] Added request cmpl-bb883940c6da482d826aadf7c11d20cb-0.
INFO 01-23 06:46:46 logger.py:37] Received request cmpl-ef0bb6dfa0a545a782773eed315159cb-0: prompt: 'Write a poem about artificial intelligence', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 33894, 922, 21075, 11478], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60722 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:46:46 engine.py:267] Added request cmpl-ef0bb6dfa0a545a782773eed315159cb-0.
INFO 01-23 06:46:46 logger.py:37] Received request cmpl-c9417412925d40d2835599119a4eb9b0-0: prompt: 'How does photosynthesis work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 4438, 1587, 7397, 74767, 990, 30], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57748 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:46:46 engine.py:267] Added request cmpl-c9417412925d40d2835599119a4eb9b0-0.
INFO 01-23 06:46:47 metrics.py:467] Avg prompt throughput: 5.8 tokens/s, Avg generation throughput: 125.0 tokens/s, Running: 18 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.6%, CPU KV cache usage: 0.0%.
INFO 01-23 06:46:49 logger.py:37] Received request cmpl-aabdffccd7094f84a20dbf3b7a9a01a5-0: prompt: 'How does photosynthesis work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 4438, 1587, 7397, 74767, 990, 30], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57752 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:46:49 engine.py:267] Added request cmpl-aabdffccd7094f84a20dbf3b7a9a01a5-0.
INFO 01-23 06:46:49 logger.py:37] Received request cmpl-f3100a20b30b4462b41a3ae9d9858ae8-0: prompt: 'Explain quantum computing in simple terms', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 31228, 25213, 304, 4382, 3878], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52506 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:46:50 engine.py:267] Added request cmpl-f3100a20b30b4462b41a3ae9d9858ae8-0.
INFO 01-23 06:46:52 logger.py:37] Received request cmpl-037c67b81e3748b4993fd8e6db448138-0: prompt: 'Write a poem about artificial intelligence', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 33894, 922, 21075, 11478], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60724 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:46:52 engine.py:267] Added request cmpl-037c67b81e3748b4993fd8e6db448138-0.
INFO 01-23 06:46:52 metrics.py:467] Avg prompt throughput: 4.4 tokens/s, Avg generation throughput: 129.3 tokens/s, Running: 19 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.8%, CPU KV cache usage: 0.0%.
INFO 01-23 06:46:53 logger.py:37] Received request cmpl-24744af42b854ef797dc903bc414666f-0: prompt: 'How does photosynthesis work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 4438, 1587, 7397, 74767, 990, 30], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46142 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:46:53 engine.py:267] Added request cmpl-24744af42b854ef797dc903bc414666f-0.
INFO 01-23 06:46:56 logger.py:37] Received request cmpl-f5f2604e23b04ee39a38aed084096cfb-0: prompt: 'How does photosynthesis work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 4438, 1587, 7397, 74767, 990, 30], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46148 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:46:56 engine.py:267] Added request cmpl-f5f2604e23b04ee39a38aed084096cfb-0.
INFO 01-23 06:46:57 logger.py:37] Received request cmpl-117a816e3b8b44a3810f1e96c6df727c-0: prompt: 'Explain quantum computing in simple terms', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 31228, 25213, 304, 4382, 3878], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41340 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:46:57 engine.py:267] Added request cmpl-117a816e3b8b44a3810f1e96c6df727c-0.
INFO 01-23 06:46:57 metrics.py:467] Avg prompt throughput: 4.4 tokens/s, Avg generation throughput: 132.7 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 01-23 06:46:58 logger.py:37] Received request cmpl-fbc65e40479c46d6a9d0fb8c14360d58-0: prompt: 'Write a poem about artificial intelligence', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 33894, 922, 21075, 11478], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39008 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:46:58 engine.py:267] Added request cmpl-fbc65e40479c46d6a9d0fb8c14360d58-0.
INFO 01-23 06:47:00 logger.py:37] Received request cmpl-e23ae640167347f49a0b4a7bddc16fe4-0: prompt: 'How does photosynthesis work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 4438, 1587, 7397, 74767, 990, 30], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46158 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:00 engine.py:267] Added request cmpl-e23ae640167347f49a0b4a7bddc16fe4-0.
INFO 01-23 06:47:02 metrics.py:467] Avg prompt throughput: 2.8 tokens/s, Avg generation throughput: 139.5 tokens/s, Running: 20 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 01-23 06:47:04 logger.py:37] Received request cmpl-592e1ec889d446a9a7e385cd4e3ce6ff-0: prompt: 'Write a poem about artificial intelligence', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 33894, 922, 21075, 11478], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56690 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:04 engine.py:267] Added request cmpl-592e1ec889d446a9a7e385cd4e3ce6ff-0.
INFO 01-23 06:47:05 logger.py:37] Received request cmpl-a2a0808aaed64243bd7f0e8d560d83f9-0: prompt: 'How does photosynthesis work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 4438, 1587, 7397, 74767, 990, 30], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44214 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:05 engine.py:267] Added request cmpl-a2a0808aaed64243bd7f0e8d560d83f9-0.
INFO 01-23 06:47:05 logger.py:37] Received request cmpl-0717c2aa921f448e857786cdb82baea7-0: prompt: 'Write a short story on Cristiano Ronaldo.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 3446, 389, 100215, 65515, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55476 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:05 logger.py:37] Received request cmpl-ebadac97296e4906ad87f8eb6eb9af61-0: prompt: 'Explain quantum computing in simple terms', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 31228, 25213, 304, 4382, 3878], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34960 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:05 engine.py:267] Added request cmpl-0717c2aa921f448e857786cdb82baea7-0.
INFO 01-23 06:47:05 engine.py:267] Added request cmpl-ebadac97296e4906ad87f8eb6eb9af61-0.
INFO 01-23 06:47:07 logger.py:37] Received request cmpl-12c0023815794adcaa6398181c796933-0: prompt: 'Write a short story on Cristiano Ronaldo.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 3446, 389, 100215, 65515, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55488 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:07 engine.py:267] Added request cmpl-12c0023815794adcaa6398181c796933-0.
INFO 01-23 06:47:07 metrics.py:467] Avg prompt throughput: 8.0 tokens/s, Avg generation throughput: 141.8 tokens/s, Running: 21 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.3%, CPU KV cache usage: 0.0%.
INFO 01-23 06:47:08 logger.py:37] Received request cmpl-ec007b2b3db441b396483ad3444ae0e7-0: prompt: 'Write a short story on Cristiano Ronaldo.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 3446, 389, 100215, 65515, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55498 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:08 engine.py:267] Added request cmpl-ec007b2b3db441b396483ad3444ae0e7-0.
INFO 01-23 06:47:09 logger.py:37] Received request cmpl-0795db82796d4105b7878aeb2b798178-0: prompt: 'Write a short story on Cristiano Ronaldo.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 3446, 389, 100215, 65515, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55506 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:09 engine.py:267] Added request cmpl-0795db82796d4105b7878aeb2b798178-0.
INFO 01-23 06:47:10 logger.py:37] Received request cmpl-ac1f86b0d5814b168bc20406ab75aca8-0: prompt: 'How does photosynthesis work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 4438, 1587, 7397, 74767, 990, 30], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44222 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:10 engine.py:267] Added request cmpl-ac1f86b0d5814b168bc20406ab75aca8-0.
INFO 01-23 06:47:11 logger.py:37] Received request cmpl-29adbd487fdf41f1a0b639875807d5d4-0: prompt: 'Write a short story on Cristiano Ronaldo.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 3446, 389, 100215, 65515, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57724 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:11 engine.py:267] Added request cmpl-29adbd487fdf41f1a0b639875807d5d4-0.
INFO 01-23 06:47:11 logger.py:37] Received request cmpl-d6f9c020779148ce9472581456cd3563-0: prompt: 'Write a poem about artificial intelligence', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 33894, 922, 21075, 11478], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56696 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:11 engine.py:267] Added request cmpl-d6f9c020779148ce9472581456cd3563-0.
INFO 01-23 06:47:12 metrics.py:467] Avg prompt throughput: 8.2 tokens/s, Avg generation throughput: 145.6 tokens/s, Running: 21 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.
INFO 01-23 06:47:13 logger.py:37] Received request cmpl-4aed74fbe9e74829ad6f50eefcd277b1-0: prompt: 'Write a short story on Cristiano Ronaldo.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 3446, 389, 100215, 65515, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57736 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:13 engine.py:267] Added request cmpl-4aed74fbe9e74829ad6f50eefcd277b1-0.
INFO 01-23 06:47:14 logger.py:37] Received request cmpl-d21c2601673a4a91a0e2f7686c52905f-0: prompt: 'Explain quantum computing in simple terms', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 31228, 25213, 304, 4382, 3878], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34964 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:14 engine.py:267] Added request cmpl-d21c2601673a4a91a0e2f7686c52905f-0.
INFO 01-23 06:47:15 logger.py:37] Received request cmpl-e25ba52f52bb4d6784382d45f24a0fbe-0: prompt: 'How does photosynthesis work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 4438, 1587, 7397, 74767, 990, 30], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60722 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:15 engine.py:267] Added request cmpl-e25ba52f52bb4d6784382d45f24a0fbe-0.
INFO 01-23 06:47:15 logger.py:37] Received request cmpl-3c5b2d5e70a44cff95fc8f444a4b0aff-0: prompt: 'Write a short story on Cristiano Ronaldo.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 3446, 389, 100215, 65515, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57748 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:15 engine.py:267] Added request cmpl-3c5b2d5e70a44cff95fc8f444a4b0aff-0.
INFO 01-23 06:47:17 metrics.py:467] Avg prompt throughput: 6.6 tokens/s, Avg generation throughput: 151.3 tokens/s, Running: 22 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 01-23 06:47:18 logger.py:37] Received request cmpl-fd46f7a29eb2430dafc564678c0aa037-0: prompt: 'Write a short story on Cristiano Ronaldo.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 3446, 389, 100215, 65515, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57752 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:18 engine.py:267] Added request cmpl-fd46f7a29eb2430dafc564678c0aa037-0.
INFO 01-23 06:47:18 logger.py:37] Received request cmpl-05eb8bfecc1e41728cbd85bcb2befbe1-0: prompt: 'Write a poem about artificial intelligence', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 33894, 922, 21075, 11478], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52506 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:18 engine.py:267] Added request cmpl-05eb8bfecc1e41728cbd85bcb2befbe1-0.
INFO 01-23 06:47:20 logger.py:37] Received request cmpl-269d95499c62442b93b5f4ab01cb8e52-0: prompt: 'How does photosynthesis work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 4438, 1587, 7397, 74767, 990, 30], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60724 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:21 engine.py:267] Added request cmpl-269d95499c62442b93b5f4ab01cb8e52-0.
INFO 01-23 06:47:21 logger.py:37] Received request cmpl-7de7a801270240e98a4b83c9dca68982-0: prompt: 'Write a short story on Cristiano Ronaldo.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 3446, 389, 100215, 65515, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46142 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:21 engine.py:267] Added request cmpl-7de7a801270240e98a4b83c9dca68982-0.
INFO 01-23 06:47:22 logger.py:37] Received request cmpl-1fe101fe36ef456291fc6f86de1b5eb7-0: prompt: 'Explain quantum computing in simple terms', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 31228, 25213, 304, 4382, 3878], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57586 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:22 engine.py:267] Added request cmpl-1fe101fe36ef456291fc6f86de1b5eb7-0.
INFO 01-23 06:47:22 metrics.py:467] Avg prompt throughput: 6.3 tokens/s, Avg generation throughput: 150.7 tokens/s, Running: 23 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.2%, CPU KV cache usage: 0.0%.
INFO 01-23 06:47:25 logger.py:37] Received request cmpl-5f843aad70a64dfdab9b7495f5f97edf-0: prompt: 'Write a short story on Cristiano Ronaldo.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 3446, 389, 100215, 65515, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46148 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:25 engine.py:267] Added request cmpl-5f843aad70a64dfdab9b7495f5f97edf-0.
INFO 01-23 06:47:26 logger.py:37] Received request cmpl-5298be767c15475282e745deceac9f9d-0: prompt: 'Write a poem about artificial intelligence', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 33894, 922, 21075, 11478], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41340 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:26 engine.py:267] Added request cmpl-5298be767c15475282e745deceac9f9d-0.
INFO 01-23 06:47:26 logger.py:37] Received request cmpl-2dc7a2fb8a3b4b5991bdeaea55100f4b-0: prompt: 'How does photosynthesis work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 4438, 1587, 7397, 74767, 990, 30], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39008 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:27 engine.py:267] Added request cmpl-2dc7a2fb8a3b4b5991bdeaea55100f4b-0.
INFO 01-23 06:47:27 metrics.py:467] Avg prompt throughput: 6.1 tokens/s, Avg generation throughput: 161.8 tokens/s, Running: 23 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 01-23 06:47:29 logger.py:37] Received request cmpl-d5969c0fd26a43159ee1815a015cb22a-0: prompt: 'Write a short story on Cristiano Ronaldo.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 3446, 389, 100215, 65515, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46158 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:29 engine.py:267] Added request cmpl-d5969c0fd26a43159ee1815a015cb22a-0.
INFO 01-23 06:47:32 logger.py:37] Received request cmpl-ff9a6ae48a05448cbfb02469280464f8-0: prompt: 'Explain quantum computing in simple terms', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 31228, 25213, 304, 4382, 3878], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57386 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:32 engine.py:267] Added request cmpl-ff9a6ae48a05448cbfb02469280464f8-0.
INFO 01-23 06:47:33 metrics.py:467] Avg prompt throughput: 3.4 tokens/s, Avg generation throughput: 161.3 tokens/s, Running: 24 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.0%, CPU KV cache usage: 0.0%.
INFO 01-23 06:47:33 logger.py:37] Received request cmpl-7e2c3899e2044ade9b215ac333178527-0: prompt: 'How does photosynthesis work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 4438, 1587, 7397, 74767, 990, 30], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56690 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:33 engine.py:267] Added request cmpl-7e2c3899e2044ade9b215ac333178527-0.
INFO 01-23 06:47:34 logger.py:37] Received request cmpl-e31ca8ed8c5e409b94a2cfa5db4b5209-0: prompt: 'Write a short story on Cristiano Ronaldo.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 3446, 389, 100215, 65515, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44214 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:34 engine.py:267] Added request cmpl-e31ca8ed8c5e409b94a2cfa5db4b5209-0.
INFO 01-23 06:47:34 logger.py:37] Received request cmpl-a06a01e97cdc4f34bd6b4def08cd5def-0: prompt: 'Write a poem about artificial intelligence', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 33894, 922, 21075, 11478], lora_request: None, prompt_adapter_request: None.
INFO 01-23 06:47:34 logger.py:37] Received request cmpl-7ba1880a44774246a178b6ae6be8ad46-0: prompt: 'Write a short description of artificial neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 21075, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34960 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:55476 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:34 engine.py:267] Added request cmpl-a06a01e97cdc4f34bd6b4def08cd5def-0.
INFO 01-23 06:47:34 engine.py:267] Added request cmpl-7ba1880a44774246a178b6ae6be8ad46-0.
INFO 01-23 06:47:36 logger.py:37] Received request cmpl-1259dbb24a7742d6aca3fb3161e98d60-0: prompt: 'Write a short description of artificial neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 21075, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55488 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:36 engine.py:267] Added request cmpl-1259dbb24a7742d6aca3fb3161e98d60-0.
INFO 01-23 06:47:36 logger.py:37] Received request cmpl-07f73f822e96403c84ab871f947a3e68-0: prompt: 'Write a short description of artificial neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 21075, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55498 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:37 engine.py:267] Added request cmpl-07f73f822e96403c84ab871f947a3e68-0.
INFO 01-23 06:47:38 metrics.py:467] Avg prompt throughput: 10.5 tokens/s, Avg generation throughput: 165.5 tokens/s, Running: 24 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%.
INFO 01-23 06:47:38 logger.py:37] Received request cmpl-ba620aa165da42a296ff7080a68e7991-0: prompt: 'Write a short description of artificial neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 21075, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55506 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:38 engine.py:267] Added request cmpl-ba620aa165da42a296ff7080a68e7991-0.
INFO 01-23 06:47:38 logger.py:37] Received request cmpl-17b0ffe8f23f428486b515874855ce74-0: prompt: 'Write a short story on Cristiano Ronaldo.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 3446, 389, 100215, 65515, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44222 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:39 engine.py:267] Added request cmpl-17b0ffe8f23f428486b515874855ce74-0.
INFO 01-23 06:47:39 logger.py:37] Received request cmpl-dedd17a837544aedb157ebf6f8cdd328-0: prompt: 'Write a short description of artificial neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 21075, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57724 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:40 engine.py:267] Added request cmpl-dedd17a837544aedb157ebf6f8cdd328-0.
INFO 01-23 06:47:40 logger.py:37] Received request cmpl-dc5f28063b85442a9f38d4701cb0b4de-0: prompt: 'How does photosynthesis work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 4438, 1587, 7397, 74767, 990, 30], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56696 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:40 engine.py:267] Added request cmpl-dc5f28063b85442a9f38d4701cb0b4de-0.
INFO 01-23 06:47:41 logger.py:37] Received request cmpl-f7f1ac2aaca844f4bb13b3f06f8c7a97-0: prompt: 'Explain quantum computing in simple terms', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 31228, 25213, 304, 4382, 3878], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36058 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:41 engine.py:267] Added request cmpl-f7f1ac2aaca844f4bb13b3f06f8c7a97-0.
INFO 01-23 06:47:41 logger.py:37] Received request cmpl-f7a2fba27fcb47fca6e24ef1b73f1f34-0: prompt: 'Write a short description of artificial neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 21075, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57736 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:42 engine.py:267] Added request cmpl-f7a2fba27fcb47fca6e24ef1b73f1f34-0.
INFO 01-23 06:47:42 logger.py:37] Received request cmpl-2d4ffc713f16408997ea512de7708c55-0: prompt: 'Write a poem about artificial intelligence', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 33894, 922, 21075, 11478], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34964 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:42 engine.py:267] Added request cmpl-2d4ffc713f16408997ea512de7708c55-0.
INFO 01-23 06:47:43 metrics.py:467] Avg prompt throughput: 12.1 tokens/s, Avg generation throughput: 167.2 tokens/s, Running: 25 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 01-23 06:47:44 logger.py:37] Received request cmpl-f57c2b8dcf8c42da933b304ababa352e-0: prompt: 'Write a short story on Cristiano Ronaldo.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 3446, 389, 100215, 65515, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60722 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:44 engine.py:267] Added request cmpl-f57c2b8dcf8c42da933b304ababa352e-0.
INFO 01-23 06:47:44 logger.py:37] Received request cmpl-87ee4dcb5a794b85a0014188f3fcb563-0: prompt: 'Write a short description of artificial neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 21075, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57748 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:44 engine.py:267] Added request cmpl-87ee4dcb5a794b85a0014188f3fcb563-0.
INFO 01-23 06:47:47 logger.py:37] Received request cmpl-6475ba8b024c420a9b432cfeea1e1d9e-0: prompt: 'Write a short description of artificial neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 21075, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57752 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:47 engine.py:267] Added request cmpl-6475ba8b024c420a9b432cfeea1e1d9e-0.
INFO 01-23 06:47:47 logger.py:37] Received request cmpl-ea83c49706ac4058b366b256d5257b09-0: prompt: 'How does photosynthesis work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 4438, 1587, 7397, 74767, 990, 30], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52506 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:47 engine.py:267] Added request cmpl-ea83c49706ac4058b366b256d5257b09-0.
INFO 01-23 06:47:48 metrics.py:467] Avg prompt throughput: 7.1 tokens/s, Avg generation throughput: 172.3 tokens/s, Running: 25 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 01-23 06:47:49 logger.py:37] Received request cmpl-1ede7c5a2fa34a9780e1be4219b6f101-0: prompt: 'Write a short story on Cristiano Ronaldo.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 3446, 389, 100215, 65515, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60724 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:50 engine.py:267] Added request cmpl-1ede7c5a2fa34a9780e1be4219b6f101-0.
INFO 01-23 06:47:50 logger.py:37] Received request cmpl-018dc43b020c4785aebc05e8c30479fe-0: prompt: 'Write a short description of artificial neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 21075, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46142 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:50 engine.py:267] Added request cmpl-018dc43b020c4785aebc05e8c30479fe-0.
INFO 01-23 06:47:51 logger.py:37] Received request cmpl-ec376af8df87471f94ca0f413b105fea-0: prompt: 'Explain quantum computing in simple terms', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 31228, 25213, 304, 4382, 3878], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48674 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:51 engine.py:267] Added request cmpl-ec376af8df87471f94ca0f413b105fea-0.
INFO 01-23 06:47:51 logger.py:37] Received request cmpl-aaea866a64084d1bbf2b01266b8d8ef1-0: prompt: 'Write a poem about artificial intelligence', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 33894, 922, 21075, 11478], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57586 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:51 engine.py:267] Added request cmpl-aaea866a64084d1bbf2b01266b8d8ef1-0.
INFO 01-23 06:47:53 metrics.py:467] Avg prompt throughput: 6.7 tokens/s, Avg generation throughput: 174.6 tokens/s, Running: 26 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%.
INFO 01-23 06:47:54 logger.py:37] Received request cmpl-f2db734da32c4e35b405ca04ffe41402-0: prompt: 'Write a short description of artificial neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 21075, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46148 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:54 engine.py:267] Added request cmpl-f2db734da32c4e35b405ca04ffe41402-0.
INFO 01-23 06:47:55 logger.py:37] Received request cmpl-4e68f5be26f7493c8c71989c22288c0c-0: prompt: 'How does photosynthesis work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 4438, 1587, 7397, 74767, 990, 30], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41340 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:55 engine.py:267] Added request cmpl-4e68f5be26f7493c8c71989c22288c0c-0.
INFO 01-23 06:47:55 logger.py:37] Received request cmpl-5a1be4182db9446692c6e1f847c50570-0: prompt: 'Write a short story on Cristiano Ronaldo.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 3446, 389, 100215, 65515, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39008 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:56 engine.py:267] Added request cmpl-5a1be4182db9446692c6e1f847c50570-0.
INFO 01-23 06:47:58 metrics.py:467] Avg prompt throughput: 5.1 tokens/s, Avg generation throughput: 179.0 tokens/s, Running: 26 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.0%, CPU KV cache usage: 0.0%.
INFO 01-23 06:47:58 logger.py:37] Received request cmpl-72e0015c7c3e47b68106bbe9b70729e0-0: prompt: 'Write a short description of artificial neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 21075, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46158 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:47:58 engine.py:267] Added request cmpl-72e0015c7c3e47b68106bbe9b70729e0-0.
INFO 01-23 06:48:00 logger.py:37] Received request cmpl-fafc1f5dfb4441238282e6d0a389aa1b-0: prompt: 'Write a poem about artificial intelligence', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 33894, 922, 21075, 11478], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57386 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:01 engine.py:267] Added request cmpl-fafc1f5dfb4441238282e6d0a389aa1b-0.
INFO 01-23 06:48:02 logger.py:37] Received request cmpl-d7ab4faad14141e3a02bb87ae171ee68-0: prompt: 'Explain quantum computing in simple terms', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 31228, 25213, 304, 4382, 3878], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45076 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:02 engine.py:267] Added request cmpl-d7ab4faad14141e3a02bb87ae171ee68-0.
INFO 01-23 06:48:02 logger.py:37] Received request cmpl-392300228c90460faf87e4582ff006a0-0: prompt: 'Write a short story on Cristiano Ronaldo.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 3446, 389, 100215, 65515, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56690 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:02 engine.py:267] Added request cmpl-392300228c90460faf87e4582ff006a0-0.
INFO 01-23 06:48:03 logger.py:37] Received request cmpl-993a1be807b64ca3b884dd6a6e3323f2-0: prompt: 'Write a short description of artificial neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 21075, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44214 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:03 engine.py:267] Added request cmpl-993a1be807b64ca3b884dd6a6e3323f2-0.
INFO 01-23 06:48:03 metrics.py:467] Avg prompt throughput: 8.7 tokens/s, Avg generation throughput: 180.2 tokens/s, Running: 25 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.7%, CPU KV cache usage: 0.0%.
INFO 01-23 06:48:03 logger.py:37] Received request cmpl-09bda775bfa9458e979c2b8e048c752b-0: prompt: 'How does photosynthesis work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 4438, 1587, 7397, 74767, 990, 30], lora_request: None, prompt_adapter_request: None.
INFO 01-23 06:48:03 logger.py:37] Received request cmpl-0b0d0af3adfb4bbbaf95c6629a96f58b-0: prompt: 'Write a short description of Convolutional Neural Networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 30088, 3294, 278, 61577, 39810, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34960 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:55476 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:03 engine.py:267] Added request cmpl-09bda775bfa9458e979c2b8e048c752b-0.
INFO 01-23 06:48:03 engine.py:267] Added request cmpl-0b0d0af3adfb4bbbaf95c6629a96f58b-0.
INFO 01-23 06:48:05 logger.py:37] Received request cmpl-d3ddb478a78c4b0d99b658867e5b19bc-0: prompt: 'Write a short description of Convolutional Neural Networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 30088, 3294, 278, 61577, 39810, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55488 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:05 engine.py:267] Added request cmpl-d3ddb478a78c4b0d99b658867e5b19bc-0.
INFO 01-23 06:48:05 logger.py:37] Received request cmpl-46999f2e8df045319c2e7df5a95ccc1d-0: prompt: 'Write a short description of Convolutional Neural Networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 30088, 3294, 278, 61577, 39810, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55498 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:06 engine.py:267] Added request cmpl-46999f2e8df045319c2e7df5a95ccc1d-0.
INFO 01-23 06:48:07 logger.py:37] Received request cmpl-1d7f9849dc464341a11f45bcb946d41f-0: prompt: 'Write a short description of Convolutional Neural Networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 30088, 3294, 278, 61577, 39810, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55506 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:07 engine.py:267] Added request cmpl-1d7f9849dc464341a11f45bcb946d41f-0.
INFO 01-23 06:48:08 logger.py:37] Received request cmpl-8625c1f9965842708ec29d42468f163e-0: prompt: 'Write a short description of artificial neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 21075, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44222 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:08 engine.py:267] Added request cmpl-8625c1f9965842708ec29d42468f163e-0.
INFO 01-23 06:48:08 metrics.py:467] Avg prompt throughput: 12.8 tokens/s, Avg generation throughput: 185.0 tokens/s, Running: 27 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.8%, CPU KV cache usage: 0.0%.
INFO 01-23 06:48:09 logger.py:37] Received request cmpl-a6a80e1b55854233a04aafb6aa4af9cb-0: prompt: 'Write a short description of Convolutional Neural Networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 30088, 3294, 278, 61577, 39810, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57724 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:09 engine.py:267] Added request cmpl-a6a80e1b55854233a04aafb6aa4af9cb-0.
INFO 01-23 06:48:09 logger.py:37] Received request cmpl-6a37072ec5fc486293906cb0b522e210-0: prompt: 'Write a short story on Cristiano Ronaldo.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 3446, 389, 100215, 65515, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56696 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:09 engine.py:267] Added request cmpl-6a37072ec5fc486293906cb0b522e210-0.
INFO 01-23 06:48:10 logger.py:37] Received request cmpl-38eb269bbae14fd4be6f70dcbebb8140-0: prompt: 'Write a poem about artificial intelligence', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 33894, 922, 21075, 11478], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36058 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:10 engine.py:267] Added request cmpl-38eb269bbae14fd4be6f70dcbebb8140-0.
INFO 01-23 06:48:11 logger.py:37] Received request cmpl-9ab0fe79f9134594817e68912d9e0cb3-0: prompt: 'Write a short description of Convolutional Neural Networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 30088, 3294, 278, 61577, 39810, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57736 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:11 engine.py:267] Added request cmpl-9ab0fe79f9134594817e68912d9e0cb3-0.
INFO 01-23 06:48:11 logger.py:37] Received request cmpl-188a34b49b4d4369b3990a15972c6d27-0: prompt: 'How does photosynthesis work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 4438, 1587, 7397, 74767, 990, 30], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34964 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:12 engine.py:267] Added request cmpl-188a34b49b4d4369b3990a15972c6d27-0.
INFO 01-23 06:48:12 logger.py:37] Received request cmpl-c3a1fd7fcac84c878736e0c3b729225f-0: prompt: 'Explain quantum computing in simple terms', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 31228, 25213, 304, 4382, 3878], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47802 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:12 engine.py:267] Added request cmpl-c3a1fd7fcac84c878736e0c3b729225f-0.
INFO 01-23 06:48:13 logger.py:37] Received request cmpl-4ae7bd9193fe4176967afae41e799ce0-0: prompt: 'Write a short description of artificial neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 21075, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60722 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:13 engine.py:267] Added request cmpl-4ae7bd9193fe4176967afae41e799ce0-0.
INFO 01-23 06:48:13 metrics.py:467] Avg prompt throughput: 10.7 tokens/s, Avg generation throughput: 183.5 tokens/s, Running: 27 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.4%, CPU KV cache usage: 0.0%.
INFO 01-23 06:48:13 logger.py:37] Received request cmpl-32024182bb6a4fea8a7b2f089923ea59-0: prompt: 'Write a short description of Convolutional Neural Networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 30088, 3294, 278, 61577, 39810, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57748 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:13 engine.py:267] Added request cmpl-32024182bb6a4fea8a7b2f089923ea59-0.
INFO 01-23 06:48:16 logger.py:37] Received request cmpl-4eb278ea78d94a3dae644028277143ea-0: prompt: 'Write a short description of Convolutional Neural Networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 30088, 3294, 278, 61577, 39810, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57752 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:16 engine.py:267] Added request cmpl-4eb278ea78d94a3dae644028277143ea-0.
INFO 01-23 06:48:16 logger.py:37] Received request cmpl-62118a4f35944952a3a5ce490b063f7c-0: prompt: 'Write a short story on Cristiano Ronaldo.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 3446, 389, 100215, 65515, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52506 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:16 engine.py:267] Added request cmpl-62118a4f35944952a3a5ce490b063f7c-0.
INFO 01-23 06:48:18 metrics.py:467] Avg prompt throughput: 8.4 tokens/s, Avg generation throughput: 195.5 tokens/s, Running: 28 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.0%, CPU KV cache usage: 0.0%.
INFO 01-23 06:48:19 logger.py:37] Received request cmpl-becc95f093a148f2985ad82dcf0e6825-0: prompt: 'Write a short description of artificial neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 21075, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60724 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:19 engine.py:267] Added request cmpl-becc95f093a148f2985ad82dcf0e6825-0.
INFO 01-23 06:48:19 logger.py:37] Received request cmpl-27139a69a6744a5d8ff774f58c4178d8-0: prompt: 'Write a short description of Convolutional Neural Networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 30088, 3294, 278, 61577, 39810, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46142 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:20 engine.py:267] Added request cmpl-27139a69a6744a5d8ff774f58c4178d8-0.
INFO 01-23 06:48:20 logger.py:37] Received request cmpl-08a8780146de4e4bb70c814de1bc86c1-0: prompt: 'Write a poem about artificial intelligence', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 33894, 922, 21075, 11478], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48674 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:20 engine.py:267] Added request cmpl-08a8780146de4e4bb70c814de1bc86c1-0.
INFO 01-23 06:48:20 logger.py:37] Received request cmpl-b45ff247d2494c5f9acb505b233813b6-0: prompt: 'How does photosynthesis work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 4438, 1587, 7397, 74767, 990, 30], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57586 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:20 engine.py:267] Added request cmpl-b45ff247d2494c5f9acb505b233813b6-0.
INFO 01-23 06:48:23 logger.py:37] Received request cmpl-5f2a5d92abfc4001aef6b121138f8518-0: prompt: 'Write a short description of Convolutional Neural Networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 30088, 3294, 278, 61577, 39810, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46148 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:23 engine.py:267] Added request cmpl-5f2a5d92abfc4001aef6b121138f8518-0.
INFO 01-23 06:48:23 metrics.py:467] Avg prompt throughput: 9.5 tokens/s, Avg generation throughput: 192.3 tokens/s, Running: 28 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.0%, CPU KV cache usage: 0.0%.
INFO 01-23 06:48:24 logger.py:37] Received request cmpl-5e0b0bbfb1544a368d1adec9a22c4a25-0: prompt: 'Explain quantum computing in simple terms', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 31228, 25213, 304, 4382, 3878], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55688 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:24 engine.py:267] Added request cmpl-5e0b0bbfb1544a368d1adec9a22c4a25-0.
INFO 01-23 06:48:24 logger.py:37] Received request cmpl-14c5f03c937848948091593b089681b0-0: prompt: 'Write a short story on Cristiano Ronaldo.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 3446, 389, 100215, 65515, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41340 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:24 engine.py:267] Added request cmpl-14c5f03c937848948091593b089681b0-0.
INFO 01-23 06:48:25 logger.py:37] Received request cmpl-307574c89e344bfcb2dba499384e7df4-0: prompt: 'Write a short description of artificial neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 21075, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39008 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:25 engine.py:267] Added request cmpl-307574c89e344bfcb2dba499384e7df4-0.
INFO 01-23 06:48:27 logger.py:37] Received request cmpl-f51c2e562572467193bde607144bc1e6-0: prompt: 'Write a short description of Convolutional Neural Networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 30088, 3294, 278, 61577, 39810, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46158 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:27 engine.py:267] Added request cmpl-f51c2e562572467193bde607144bc1e6-0.
INFO 01-23 06:48:28 metrics.py:467] Avg prompt throughput: 7.7 tokens/s, Avg generation throughput: 199.0 tokens/s, Running: 29 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.4%, CPU KV cache usage: 0.0%.
INFO 01-23 06:48:30 logger.py:37] Received request cmpl-ea88f9b403c54ea79f56bef1f9d79dac-0: prompt: 'How does photosynthesis work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 4438, 1587, 7397, 74767, 990, 30], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57386 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:30 engine.py:267] Added request cmpl-ea88f9b403c54ea79f56bef1f9d79dac-0.
INFO 01-23 06:48:31 logger.py:37] Received request cmpl-c643a28bfa7b498098011a8b24789738-0: prompt: 'Write a poem about artificial intelligence', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 33894, 922, 21075, 11478], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45076 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:31 engine.py:267] Added request cmpl-c643a28bfa7b498098011a8b24789738-0.
INFO 01-23 06:48:31 logger.py:37] Received request cmpl-6fd60e75435840368baf4e533dd687e5-0: prompt: 'Write a short description of artificial neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 21075, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56690 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:31 engine.py:267] Added request cmpl-6fd60e75435840368baf4e533dd687e5-0.
INFO 01-23 06:48:32 logger.py:37] Received request cmpl-4b4aa6db2d3e4e8e9fe9a5d36c3ef987-0: prompt: 'Write a short description of Convolutional Neural Networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 30088, 3294, 278, 61577, 39810, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44214 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:32 engine.py:267] Added request cmpl-4b4aa6db2d3e4e8e9fe9a5d36c3ef987-0.
INFO 01-23 06:48:32 logger.py:37] Received request cmpl-42525d806efd457e946b0800bc264730-0: prompt: 'Write a short story on Cristiano Ronaldo.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 3446, 389, 100215, 65515, 13], lora_request: None, prompt_adapter_request: None.
INFO 01-23 06:48:32 logger.py:37] Received request cmpl-861a76858e134f73a8469dd889842a9b-0: prompt: 'Write a short description of the transformer architecture in neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 279, 43678, 18112, 304, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34960 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:55476 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:32 engine.py:267] Added request cmpl-42525d806efd457e946b0800bc264730-0.
INFO 01-23 06:48:32 engine.py:267] Added request cmpl-861a76858e134f73a8469dd889842a9b-0.
INFO 01-23 06:48:33 metrics.py:467] Avg prompt throughput: 11.4 tokens/s, Avg generation throughput: 197.7 tokens/s, Running: 29 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.3%, CPU KV cache usage: 0.0%.
INFO 01-23 06:48:34 logger.py:37] Received request cmpl-7ad4fcd38af64b8e8974d8bce3173aaf-0: prompt: 'Write a short description of the transformer architecture in neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 279, 43678, 18112, 304, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55488 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:34 engine.py:267] Added request cmpl-7ad4fcd38af64b8e8974d8bce3173aaf-0.
INFO 01-23 06:48:35 logger.py:37] Received request cmpl-0e0ffd74bed445f1b5099fcbb9ac51a1-0: prompt: 'Write a short description of the transformer architecture in neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 279, 43678, 18112, 304, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55498 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:35 engine.py:267] Added request cmpl-0e0ffd74bed445f1b5099fcbb9ac51a1-0.
INFO 01-23 06:48:35 logger.py:37] Received request cmpl-355f0dd8f7b940b1914deb3846c6f558-0: prompt: 'Explain quantum computing in simple terms', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 31228, 25213, 304, 4382, 3878], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35452 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:35 engine.py:267] Added request cmpl-355f0dd8f7b940b1914deb3846c6f558-0.
INFO 01-23 06:48:36 logger.py:37] Received request cmpl-4bdae29180b24108ac61d956357f961b-0: prompt: 'Write a short description of the transformer architecture in neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 279, 43678, 18112, 304, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55506 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:36 engine.py:267] Added request cmpl-4bdae29180b24108ac61d956357f961b-0.
INFO 01-23 06:48:37 logger.py:37] Received request cmpl-9dee4eff29a746db97393c61e2ad04eb-0: prompt: 'Write a short description of Convolutional Neural Networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 30088, 3294, 278, 61577, 39810, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44222 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:37 engine.py:267] Added request cmpl-9dee4eff29a746db97393c61e2ad04eb-0.
INFO 01-23 06:48:38 logger.py:37] Received request cmpl-9f1b92ab7449477090d75fbb34c6b162-0: prompt: 'Write a short description of the transformer architecture in neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 279, 43678, 18112, 304, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57724 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:38 engine.py:267] Added request cmpl-9f1b92ab7449477090d75fbb34c6b162-0.
INFO 01-23 06:48:38 logger.py:37] Received request cmpl-e2d1585de0e1402b9bd760e68686b79d-0: prompt: 'Write a short description of artificial neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 21075, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56696 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:38 engine.py:267] Added request cmpl-e2d1585de0e1402b9bd760e68686b79d-0.
INFO 01-23 06:48:39 metrics.py:467] Avg prompt throughput: 16.1 tokens/s, Avg generation throughput: 202.9 tokens/s, Running: 30 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.1%, CPU KV cache usage: 0.0%.
INFO 01-23 06:48:39 logger.py:37] Received request cmpl-a81b73f7d8354dd39c4d9c59772f845f-0: prompt: 'How does photosynthesis work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 4438, 1587, 7397, 74767, 990, 30], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36058 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:39 engine.py:267] Added request cmpl-a81b73f7d8354dd39c4d9c59772f845f-0.
INFO 01-23 06:48:40 logger.py:37] Received request cmpl-a2674949352a42e58c83387178deb69b-0: prompt: 'Write a short description of the transformer architecture in neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 279, 43678, 18112, 304, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57736 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:40 engine.py:267] Added request cmpl-a2674949352a42e58c83387178deb69b-0.
INFO 01-23 06:48:41 logger.py:37] Received request cmpl-ac41fad187c34b18bee1eff24c4552ca-0: prompt: 'Write a short story on Cristiano Ronaldo.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 3446, 389, 100215, 65515, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34964 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:41 engine.py:267] Added request cmpl-ac41fad187c34b18bee1eff24c4552ca-0.
INFO 01-23 06:48:41 logger.py:37] Received request cmpl-d1f9ca819edc48678fee4babad526b49-0: prompt: 'Write a poem about artificial intelligence', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 33894, 922, 21075, 11478], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47802 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:42 engine.py:267] Added request cmpl-d1f9ca819edc48678fee4babad526b49-0.
INFO 01-23 06:48:42 logger.py:37] Received request cmpl-79b3394eed854c98bbb8ba228e97a3d5-0: prompt: 'Write a short description of Convolutional Neural Networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 30088, 3294, 278, 61577, 39810, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60722 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:42 engine.py:267] Added request cmpl-79b3394eed854c98bbb8ba228e97a3d5-0.
INFO 01-23 06:48:42 logger.py:37] Received request cmpl-f3df946ba9834584ad7ea9e395ee62ca-0: prompt: 'Write a short description of the transformer architecture in neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 279, 43678, 18112, 304, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57748 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:42 engine.py:267] Added request cmpl-f3df946ba9834584ad7ea9e395ee62ca-0.
INFO 01-23 06:48:44 metrics.py:467] Avg prompt throughput: 12.0 tokens/s, Avg generation throughput: 205.6 tokens/s, Running: 30 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.9%, CPU KV cache usage: 0.0%.
INFO 01-23 06:48:45 logger.py:37] Received request cmpl-a00b48b691704ab48b3b1c32b1d31325-0: prompt: 'Write a short description of the transformer architecture in neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 279, 43678, 18112, 304, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57752 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:45 engine.py:267] Added request cmpl-a00b48b691704ab48b3b1c32b1d31325-0.
INFO 01-23 06:48:45 logger.py:37] Received request cmpl-0391cd5471044a7d9be2c8d5d9cb188a-0: prompt: 'Write a short description of artificial neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 21075, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52506 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:46 engine.py:267] Added request cmpl-0391cd5471044a7d9be2c8d5d9cb188a-0.
INFO 01-23 06:48:47 logger.py:37] Received request cmpl-0d1b5de5b71049f0bec8987280ad9382-0: prompt: 'Explain quantum computing in simple terms', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 31228, 25213, 304, 4382, 3878], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59632 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:47 engine.py:267] Added request cmpl-0d1b5de5b71049f0bec8987280ad9382-0.
INFO 01-23 06:48:48 logger.py:37] Received request cmpl-c6d08e4b101343d684399f435073c65a-0: prompt: 'Write a short description of Convolutional Neural Networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 30088, 3294, 278, 61577, 39810, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60724 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:48 engine.py:267] Added request cmpl-c6d08e4b101343d684399f435073c65a-0.
INFO 01-23 06:48:49 logger.py:37] Received request cmpl-f564d63ba0b8413c80cd01e22754a944-0: prompt: 'Write a short description of the transformer architecture in neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 279, 43678, 18112, 304, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46142 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:49 engine.py:267] Added request cmpl-f564d63ba0b8413c80cd01e22754a944-0.
INFO 01-23 06:48:49 metrics.py:467] Avg prompt throughput: 8.4 tokens/s, Avg generation throughput: 205.2 tokens/s, Running: 31 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.2%, CPU KV cache usage: 0.0%.
INFO 01-23 06:48:49 logger.py:37] Received request cmpl-4975fb2603024e4db109bc5005767040-0: prompt: 'How does photosynthesis work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 4438, 1587, 7397, 74767, 990, 30], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48674 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:49 engine.py:267] Added request cmpl-4975fb2603024e4db109bc5005767040-0.
INFO 01-23 06:48:50 logger.py:37] Received request cmpl-d460a6ea7b0d40aead331e0aa2630a54-0: prompt: 'Write a short story on Cristiano Ronaldo.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 3446, 389, 100215, 65515, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57586 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:50 engine.py:267] Added request cmpl-d460a6ea7b0d40aead331e0aa2630a54-0.
INFO 01-23 06:48:52 logger.py:37] Received request cmpl-a7adb823d7374bb58ef311cdab031474-0: prompt: 'Write a short description of the transformer architecture in neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 279, 43678, 18112, 304, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46148 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:52 engine.py:267] Added request cmpl-a7adb823d7374bb58ef311cdab031474-0.
INFO 01-23 06:48:53 logger.py:37] Received request cmpl-d8565de4d6d54f6998f8099433a8585b-0: prompt: 'Write a poem about artificial intelligence', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 33894, 922, 21075, 11478], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55688 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:53 engine.py:267] Added request cmpl-d8565de4d6d54f6998f8099433a8585b-0.
INFO 01-23 06:48:53 logger.py:37] Received request cmpl-581edbe543fd47aea99fae5df77db5f3-0: prompt: 'Write a short description of artificial neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 21075, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41340 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:53 engine.py:267] Added request cmpl-581edbe543fd47aea99fae5df77db5f3-0.
INFO 01-23 06:48:54 metrics.py:467] Avg prompt throughput: 11.8 tokens/s, Avg generation throughput: 215.3 tokens/s, Running: 30 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.1%, CPU KV cache usage: 0.0%.
INFO 01-23 06:48:54 logger.py:37] Received request cmpl-01f2b9933cbc43f3bb02f0a7811f3a1f-0: prompt: 'Write a short description of Convolutional Neural Networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 30088, 3294, 278, 61577, 39810, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39008 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:54 engine.py:267] Added request cmpl-01f2b9933cbc43f3bb02f0a7811f3a1f-0.
INFO 01-23 06:48:56 logger.py:37] Received request cmpl-bbb70f1bfc7c4abfba9a760694e08384-0: prompt: 'Write a short description of the transformer architecture in neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 279, 43678, 18112, 304, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46158 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:57 engine.py:267] Added request cmpl-bbb70f1bfc7c4abfba9a760694e08384-0.
INFO 01-23 06:48:59 logger.py:37] Received request cmpl-52e7231f40f14f43b28ecf1d94bf43f3-0: prompt: 'Write a short story on Cristiano Ronaldo.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 3446, 389, 100215, 65515, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57386 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:48:59 engine.py:267] Added request cmpl-52e7231f40f14f43b28ecf1d94bf43f3-0.
INFO 01-23 06:48:59 metrics.py:467] Avg prompt throughput: 4.9 tokens/s, Avg generation throughput: 210.1 tokens/s, Running: 31 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.7%, CPU KV cache usage: 0.0%.
INFO 01-23 06:49:00 logger.py:37] Received request cmpl-bcbb0da604244aa0aeed70a9ff4d43e6-0: prompt: 'Explain quantum computing in simple terms', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 31228, 25213, 304, 4382, 3878], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47698 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:00 engine.py:267] Added request cmpl-bcbb0da604244aa0aeed70a9ff4d43e6-0.
INFO 01-23 06:49:00 logger.py:37] Received request cmpl-c2c27625dfab4873a3a9ee9bc66cb6d8-0: prompt: 'How does photosynthesis work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 4438, 1587, 7397, 74767, 990, 30], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45076 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:00 engine.py:267] Added request cmpl-c2c27625dfab4873a3a9ee9bc66cb6d8-0.
INFO 01-23 06:49:00 logger.py:37] Received request cmpl-d58f0856a2be496e884dc2db1660ffb0-0: prompt: 'Write a short description of Convolutional Neural Networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 30088, 3294, 278, 61577, 39810, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56690 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:00 engine.py:267] Added request cmpl-d58f0856a2be496e884dc2db1660ffb0-0.
INFO 01-23 06:49:01 logger.py:37] Received request cmpl-6ae1f63a133343818bb4d4af2891e0ee-0: prompt: 'Write a short description of the transformer architecture in neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 279, 43678, 18112, 304, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44214 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:01 engine.py:267] Added request cmpl-6ae1f63a133343818bb4d4af2891e0ee-0.
INFO 01-23 06:49:01 logger.py:37] Received request cmpl-4ed583d85c7c4b1190b3c0a3a675df42-0: prompt: 'Write a short description of diffusion models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 58430, 4211, 13], lora_request: None, prompt_adapter_request: None.
INFO 01-23 06:49:01 logger.py:37] Received request cmpl-b4666ffbd2c7468fb54a6d3f9a596337-0: prompt: 'Write a short description of artificial neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 21075, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55476 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:34960 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:01 engine.py:267] Added request cmpl-4ed583d85c7c4b1190b3c0a3a675df42-0.
INFO 01-23 06:49:01 engine.py:267] Added request cmpl-b4666ffbd2c7468fb54a6d3f9a596337-0.
INFO 01-23 06:49:03 logger.py:37] Received request cmpl-f206b0c82de140869c056aa614f60613-0: prompt: 'Write a short description of diffusion models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 58430, 4211, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55488 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:03 engine.py:267] Added request cmpl-f206b0c82de140869c056aa614f60613-0.
INFO 01-23 06:49:04 logger.py:37] Received request cmpl-ac98e92862a84f28833a7b17c3f99542-0: prompt: 'Write a short description of diffusion models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 58430, 4211, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55498 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:04 engine.py:267] Added request cmpl-ac98e92862a84f28833a7b17c3f99542-0.
INFO 01-23 06:49:04 metrics.py:467] Avg prompt throughput: 15.1 tokens/s, Avg generation throughput: 217.0 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.5%, CPU KV cache usage: 0.0%.
INFO 01-23 06:49:04 logger.py:37] Received request cmpl-84fba316769b4bb3a2dd33e32a999142-0: prompt: 'Write a poem about artificial intelligence', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 33894, 922, 21075, 11478], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35452 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:04 engine.py:267] Added request cmpl-84fba316769b4bb3a2dd33e32a999142-0.
INFO 01-23 06:49:05 logger.py:37] Received request cmpl-4d494e5589bb4e8891a3e659a732fbd0-0: prompt: 'Write a short description of diffusion models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 58430, 4211, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55506 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:05 engine.py:267] Added request cmpl-4d494e5589bb4e8891a3e659a732fbd0-0.
INFO 01-23 06:49:06 logger.py:37] Received request cmpl-c9a243f5f69e4488974c1c9812cc37dc-0: prompt: 'Write a short description of the transformer architecture in neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 279, 43678, 18112, 304, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44222 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:06 engine.py:267] Added request cmpl-c9a243f5f69e4488974c1c9812cc37dc-0.
INFO 01-23 06:49:07 logger.py:37] Received request cmpl-5e1258edd16d479f993993e60ea2cc20-0: prompt: 'Write a short description of diffusion models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 58430, 4211, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57724 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:07 engine.py:267] Added request cmpl-5e1258edd16d479f993993e60ea2cc20-0.
INFO 01-23 06:49:07 logger.py:37] Received request cmpl-7fca207b358c49b8bcb32edfc51c284b-0: prompt: 'Write a short description of Convolutional Neural Networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 30088, 3294, 278, 61577, 39810, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56696 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:07 engine.py:267] Added request cmpl-7fca207b358c49b8bcb32edfc51c284b-0.
INFO 01-23 06:49:08 logger.py:37] Received request cmpl-a2a6e1f37c3e4290ac12b38a7248277e-0: prompt: 'Write a short story on Cristiano Ronaldo.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 3446, 389, 100215, 65515, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36058 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:09 engine.py:267] Added request cmpl-a2a6e1f37c3e4290ac12b38a7248277e-0.
INFO 01-23 06:49:09 logger.py:37] Received request cmpl-be85812510054742bf8769c0df2c3895-0: prompt: 'Write a short description of diffusion models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 58430, 4211, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57736 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:09 engine.py:267] Added request cmpl-be85812510054742bf8769c0df2c3895-0.
INFO 01-23 06:49:09 metrics.py:467] Avg prompt throughput: 13.3 tokens/s, Avg generation throughput: 218.4 tokens/s, Running: 32 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.2%, CPU KV cache usage: 0.0%.
INFO 01-23 06:49:10 logger.py:37] Received request cmpl-60d12034d53b469995534ce73376cf5b-0: prompt: 'Write a short description of artificial neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 21075, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34964 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:10 engine.py:267] Added request cmpl-60d12034d53b469995534ce73376cf5b-0.
INFO 01-23 06:49:11 logger.py:37] Received request cmpl-c0e6d29ac4584686b7dcdcd753f7bf60-0: prompt: 'How does photosynthesis work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 4438, 1587, 7397, 74767, 990, 30], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47802 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:11 engine.py:267] Added request cmpl-c0e6d29ac4584686b7dcdcd753f7bf60-0.
INFO 01-23 06:49:11 logger.py:37] Received request cmpl-f383a76853aa424986fc620ff29580b7-0: prompt: 'Write a short description of the transformer architecture in neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 279, 43678, 18112, 304, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60722 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:11 engine.py:267] Added request cmpl-f383a76853aa424986fc620ff29580b7-0.
INFO 01-23 06:49:11 logger.py:37] Received request cmpl-b9d47f545e6841cca62bc83270006da0-0: prompt: 'Write a short description of diffusion models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 58430, 4211, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57748 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:12 engine.py:267] Added request cmpl-b9d47f545e6841cca62bc83270006da0-0.
INFO 01-23 06:49:12 logger.py:37] Received request cmpl-4a50c1be0edf4827adb2ed06137ba750-0: prompt: 'Explain quantum computing in simple terms', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 31228, 25213, 304, 4382, 3878], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41556 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:12 engine.py:267] Added request cmpl-4a50c1be0edf4827adb2ed06137ba750-0.
INFO 01-23 06:49:14 metrics.py:467] Avg prompt throughput: 11.1 tokens/s, Avg generation throughput: 224.5 tokens/s, Running: 33 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.5%, CPU KV cache usage: 0.0%.
INFO 01-23 06:49:14 logger.py:37] Received request cmpl-985a1bce8ac7485485e1a33cc47d83b4-0: prompt: 'Write a short description of diffusion models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 58430, 4211, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57752 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:15 engine.py:267] Added request cmpl-985a1bce8ac7485485e1a33cc47d83b4-0.
INFO 01-23 06:49:15 logger.py:37] Received request cmpl-05bcada416ca4c448a9db06153eadb72-0: prompt: 'Write a short description of Convolutional Neural Networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 30088, 3294, 278, 61577, 39810, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52506 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:15 engine.py:267] Added request cmpl-05bcada416ca4c448a9db06153eadb72-0.
INFO 01-23 06:49:16 logger.py:37] Received request cmpl-ea5ba04492fa4ffcb3e3615cdeadbb2a-0: prompt: 'Write a poem about artificial intelligence', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 33894, 922, 21075, 11478], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59632 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:16 engine.py:267] Added request cmpl-ea5ba04492fa4ffcb3e3615cdeadbb2a-0.
INFO 01-23 06:49:17 logger.py:37] Received request cmpl-72ee55e5d108470dbdfebf89e4562ef7-0: prompt: 'Write a short description of the transformer architecture in neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 279, 43678, 18112, 304, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60724 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:17 engine.py:267] Added request cmpl-72ee55e5d108470dbdfebf89e4562ef7-0.
INFO 01-23 06:49:18 logger.py:37] Received request cmpl-c9571b79dbc74668a2409c177da5180c-0: prompt: 'Write a short description of diffusion models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 58430, 4211, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46142 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:18 engine.py:267] Added request cmpl-c9571b79dbc74668a2409c177da5180c-0.
INFO 01-23 06:49:19 logger.py:37] Received request cmpl-28522d19d4274aa9957cbfeef37d76f7-0: prompt: 'Write a short story on Cristiano Ronaldo.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 3446, 389, 100215, 65515, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48674 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:19 engine.py:267] Added request cmpl-28522d19d4274aa9957cbfeef37d76f7-0.
INFO 01-23 06:49:19 logger.py:37] Received request cmpl-4c3e705241dd4f218e29a7cfc8885cdb-0: prompt: 'Write a short description of artificial neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 21075, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57586 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:19 engine.py:267] Added request cmpl-4c3e705241dd4f218e29a7cfc8885cdb-0.
INFO 01-23 06:49:19 metrics.py:467] Avg prompt throughput: 13.5 tokens/s, Avg generation throughput: 224.8 tokens/s, Running: 33 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.3%, CPU KV cache usage: 0.0%.
INFO 01-23 06:49:21 logger.py:37] Received request cmpl-b0c2bb5fe9e9462788432c4cedf2fcce-0: prompt: 'Write a short description of diffusion models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 58430, 4211, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46148 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:22 engine.py:267] Added request cmpl-b0c2bb5fe9e9462788432c4cedf2fcce-0.
INFO 01-23 06:49:22 logger.py:37] Received request cmpl-149871e8543e44e48915de27ca16cfa2-0: prompt: 'How does photosynthesis work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 4438, 1587, 7397, 74767, 990, 30], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55688 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:22 engine.py:267] Added request cmpl-149871e8543e44e48915de27ca16cfa2-0.
INFO 01-23 06:49:22 logger.py:37] Received request cmpl-4dd72c39e39e45399095eb6b14762635-0: prompt: 'Write a short description of Convolutional Neural Networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 30088, 3294, 278, 61577, 39810, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41340 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:22 engine.py:267] Added request cmpl-4dd72c39e39e45399095eb6b14762635-0.
INFO 01-23 06:49:23 logger.py:37] Received request cmpl-5f6e247c3fc24e1880d9d3be1c2e6366-0: prompt: 'Write a short description of the transformer architecture in neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 279, 43678, 18112, 304, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39008 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:23 engine.py:267] Added request cmpl-5f6e247c3fc24e1880d9d3be1c2e6366-0.
INFO 01-23 06:49:24 metrics.py:467] Avg prompt throughput: 8.0 tokens/s, Avg generation throughput: 225.6 tokens/s, Running: 33 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.5%, CPU KV cache usage: 0.0%.
INFO 01-23 06:49:26 logger.py:37] Received request cmpl-b2779c620f954eaabaada90593ca705f-0: prompt: 'Explain quantum computing in simple terms', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 31228, 25213, 304, 4382, 3878], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50058 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:26 engine.py:267] Added request cmpl-b2779c620f954eaabaada90593ca705f-0.
INFO 01-23 06:49:26 logger.py:37] Received request cmpl-841e0cb2376d4ffa897477242da80cc8-0: prompt: 'Write a short description of diffusion models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 58430, 4211, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46158 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:26 engine.py:267] Added request cmpl-841e0cb2376d4ffa897477242da80cc8-0.
INFO 01-23 06:49:28 logger.py:37] Received request cmpl-2f2d13e5c1404cb289829bbe904f25bb-0: prompt: 'Write a short description of artificial neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 21075, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57386 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:28 engine.py:267] Added request cmpl-2f2d13e5c1404cb289829bbe904f25bb-0.
INFO 01-23 06:49:29 logger.py:37] Received request cmpl-b4402d39db924917845f4c80e49ecdbb-0: prompt: 'Write a poem about artificial intelligence', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 33894, 922, 21075, 11478], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47698 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:29 engine.py:267] Added request cmpl-b4402d39db924917845f4c80e49ecdbb-0.
INFO 01-23 06:49:29 logger.py:37] Received request cmpl-80499b28e94f4b26a04f4be6ea63d124-0: prompt: 'Write a short story on Cristiano Ronaldo.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 3446, 389, 100215, 65515, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45076 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:29 engine.py:267] Added request cmpl-80499b28e94f4b26a04f4be6ea63d124-0.
INFO 01-23 06:49:29 metrics.py:467] Avg prompt throughput: 8.4 tokens/s, Avg generation throughput: 230.3 tokens/s, Running: 34 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.0%, CPU KV cache usage: 0.0%.
INFO 01-23 06:49:30 logger.py:37] Received request cmpl-cf32a06571bb4e82b3ebc2416366ff6a-0: prompt: 'Write a short description of the transformer architecture in neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 279, 43678, 18112, 304, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56690 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:30 engine.py:267] Added request cmpl-cf32a06571bb4e82b3ebc2416366ff6a-0.
INFO 01-23 06:49:30 logger.py:37] Received request cmpl-3d1f4530b1c7461ab0bb14c4bda51d2b-0: prompt: 'Write a short description of diffusion models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 58430, 4211, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44214 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:30 engine.py:267] Added request cmpl-3d1f4530b1c7461ab0bb14c4bda51d2b-0.
INFO 01-23 06:49:31 logger.py:37] Received request cmpl-be207013835d42a8b69cfcdb32273cc3-0: prompt: 'Write a short description of Convolutional Neural Networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 30088, 3294, 278, 61577, 39810, 13], lora_request: None, prompt_adapter_request: None.
INFO 01-23 06:49:31 logger.py:37] Received request cmpl-c5980f3253db4ae79d4157a1e473b899-0: prompt: 'Write a short description of recommendation systems using neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 28782, 6067, 1701, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34960 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:55476 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:31 engine.py:267] Added request cmpl-be207013835d42a8b69cfcdb32273cc3-0.
INFO 01-23 06:49:31 engine.py:267] Added request cmpl-c5980f3253db4ae79d4157a1e473b899-0.
INFO 01-23 06:49:32 logger.py:37] Received request cmpl-08f0fc93a3c94b7eaf75f6051f52f5dc-0: prompt: 'Write a short description of recommendation systems using neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 28782, 6067, 1701, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55488 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:32 engine.py:267] Added request cmpl-08f0fc93a3c94b7eaf75f6051f52f5dc-0.
INFO 01-23 06:49:33 logger.py:37] Received request cmpl-e96cf43ea34340b4b179968e5cf240e9-0: prompt: 'Write a short description of recommendation systems using neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 28782, 6067, 1701, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55498 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:33 engine.py:267] Added request cmpl-e96cf43ea34340b4b179968e5cf240e9-0.
INFO 01-23 06:49:34 logger.py:37] Received request cmpl-145565c308324d17b1bd9434a9482963-0: prompt: 'How does photosynthesis work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 4438, 1587, 7397, 74767, 990, 30], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35452 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:34 engine.py:267] Added request cmpl-145565c308324d17b1bd9434a9482963-0.
INFO 01-23 06:49:34 logger.py:37] Received request cmpl-658c72b4db9a40259537c58cbe7a1db4-0: prompt: 'Write a short description of diffusion models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 58430, 4211, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39008 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:34 engine.py:267] Added request cmpl-658c72b4db9a40259537c58cbe7a1db4-0.
INFO 01-23 06:49:35 metrics.py:467] Avg prompt throughput: 15.2 tokens/s, Avg generation throughput: 226.6 tokens/s, Running: 33 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.5%, CPU KV cache usage: 0.0%.
INFO 01-23 06:49:35 logger.py:37] Received request cmpl-fba11fc7565545a69986fc5fe26a649d-0: prompt: 'Write a short description of recommendation systems using neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 28782, 6067, 1701, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55506 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:35 engine.py:267] Added request cmpl-fba11fc7565545a69986fc5fe26a649d-0.
INFO 01-23 06:49:35 logger.py:37] Received request cmpl-9a9d36ea4f7f4a5d981a3089a43d9e63-0: prompt: 'Write a short description of diffusion models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 58430, 4211, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44222 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:35 engine.py:267] Added request cmpl-9a9d36ea4f7f4a5d981a3089a43d9e63-0.
INFO 01-23 06:49:36 logger.py:37] Received request cmpl-90f44f7814ed4962ad0be67435c7140b-0: prompt: 'Write a short description of recommendation systems using neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 28782, 6067, 1701, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57724 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:36 engine.py:267] Added request cmpl-90f44f7814ed4962ad0be67435c7140b-0.
INFO 01-23 06:49:37 logger.py:37] Received request cmpl-8bf922591aad4e1f94ac1cba6c277949-0: prompt: 'Write a short description of the transformer architecture in neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 279, 43678, 18112, 304, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56696 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:37 engine.py:267] Added request cmpl-8bf922591aad4e1f94ac1cba6c277949-0.
INFO 01-23 06:49:38 logger.py:37] Received request cmpl-a657cb75e60b48c39076fc7d67fd5706-0: prompt: 'Write a short description of artificial neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 21075, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36058 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:38 engine.py:267] Added request cmpl-a657cb75e60b48c39076fc7d67fd5706-0.
INFO 01-23 06:49:38 logger.py:37] Received request cmpl-3b2b951803e1435b8a08826b14f75fb9-0: prompt: 'Write a short description of recommendation systems using neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 28782, 6067, 1701, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57736 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:38 engine.py:267] Added request cmpl-3b2b951803e1435b8a08826b14f75fb9-0.
INFO 01-23 06:49:39 logger.py:37] Received request cmpl-3df58d00fe594acaaf50ceabccf7276d-0: prompt: 'Explain quantum computing in simple terms', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 31228, 25213, 304, 4382, 3878], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54386 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:39 engine.py:267] Added request cmpl-3df58d00fe594acaaf50ceabccf7276d-0.
INFO 01-23 06:49:39 logger.py:37] Received request cmpl-226975f61b42485e8c1dfe5dac6762af-0: prompt: 'Write a short description of Convolutional Neural Networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 30088, 3294, 278, 61577, 39810, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34964 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:39 engine.py:267] Added request cmpl-226975f61b42485e8c1dfe5dac6762af-0.
INFO 01-23 06:49:40 metrics.py:467] Avg prompt throughput: 19.2 tokens/s, Avg generation throughput: 234.6 tokens/s, Running: 35 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.5%, CPU KV cache usage: 0.0%.
INFO 01-23 06:49:40 logger.py:37] Received request cmpl-f3763f6604ce440698c343e267735d03-0: prompt: 'Write a short story on Cristiano Ronaldo.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 3446, 389, 100215, 65515, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47802 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:40 engine.py:267] Added request cmpl-f3763f6604ce440698c343e267735d03-0.
INFO 01-23 06:49:41 logger.py:37] Received request cmpl-e145d8e2f9144486bf422c6d297446cd-0: prompt: 'Write a short description of diffusion models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 58430, 4211, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60722 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:41 engine.py:267] Added request cmpl-e145d8e2f9144486bf422c6d297446cd-0.
INFO 01-23 06:49:41 logger.py:37] Received request cmpl-c4df1ce52af94571a773d5d9131613a7-0: prompt: 'Write a short description of recommendation systems using neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 28782, 6067, 1701, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57748 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:41 engine.py:267] Added request cmpl-c4df1ce52af94571a773d5d9131613a7-0.
INFO 01-23 06:49:42 logger.py:37] Received request cmpl-6b177d5ca71f4a28a82b55bdfb5ccaf7-0: prompt: 'Write a poem about artificial intelligence', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 33894, 922, 21075, 11478], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41556 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:42 engine.py:267] Added request cmpl-6b177d5ca71f4a28a82b55bdfb5ccaf7-0.
INFO 01-23 06:49:44 logger.py:37] Received request cmpl-2c81d5bad8df4df29ec69106c28a659a-0: prompt: 'Write a short description of recommendation systems using neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 28782, 6067, 1701, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57752 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:44 engine.py:267] Added request cmpl-2c81d5bad8df4df29ec69106c28a659a-0.
INFO 01-23 06:49:44 logger.py:37] Received request cmpl-3d99c2867e7e46d78db5cf2c768c6455-0: prompt: 'Write a short description of the transformer architecture in neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 279, 43678, 18112, 304, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52506 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:44 engine.py:267] Added request cmpl-3d99c2867e7e46d78db5cf2c768c6455-0.
INFO 01-23 06:49:45 metrics.py:467] Avg prompt throughput: 12.1 tokens/s, Avg generation throughput: 238.1 tokens/s, Running: 35 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.5%, CPU KV cache usage: 0.0%.
INFO 01-23 06:49:46 logger.py:37] Received request cmpl-5a2224340b414599b1ac81fbc61b1a60-0: prompt: 'How does photosynthesis work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 4438, 1587, 7397, 74767, 990, 30], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59632 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:46 engine.py:267] Added request cmpl-5a2224340b414599b1ac81fbc61b1a60-0.
INFO 01-23 06:49:46 logger.py:37] Received request cmpl-881e8c90c7af4b40bd2dc2cf2dec87ee-0: prompt: 'Write a short description of diffusion models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 58430, 4211, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60724 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:46 engine.py:267] Added request cmpl-881e8c90c7af4b40bd2dc2cf2dec87ee-0.
INFO 01-23 06:49:47 logger.py:37] Received request cmpl-9e2177ae8a6a458b9506d7ecb75e0a72-0: prompt: 'Write a short description of recommendation systems using neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 28782, 6067, 1701, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46142 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:47 engine.py:267] Added request cmpl-9e2177ae8a6a458b9506d7ecb75e0a72-0.
INFO 01-23 06:49:48 logger.py:37] Received request cmpl-3f4d0652c4cd4d9596d23e8f0a5d8346-0: prompt: 'Write a short description of artificial neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 21075, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48674 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:48 engine.py:267] Added request cmpl-3f4d0652c4cd4d9596d23e8f0a5d8346-0.
INFO 01-23 06:49:48 logger.py:37] Received request cmpl-3f8d02e8c552478fa973105543035775-0: prompt: 'Write a short description of Convolutional Neural Networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 30088, 3294, 278, 61577, 39810, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57586 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:48 engine.py:267] Added request cmpl-3f8d02e8c552478fa973105543035775-0.
INFO 01-23 06:49:50 metrics.py:467] Avg prompt throughput: 9.8 tokens/s, Avg generation throughput: 238.5 tokens/s, Running: 35 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.7%, CPU KV cache usage: 0.0%.
INFO 01-23 06:49:51 logger.py:37] Received request cmpl-1e502d06fe36435aab36939c54e7b742-0: prompt: 'Write a short description of recommendation systems using neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 28782, 6067, 1701, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46148 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:51 engine.py:267] Added request cmpl-1e502d06fe36435aab36939c54e7b742-0.
INFO 01-23 06:49:51 logger.py:37] Received request cmpl-65adac63ab50426c927a5e8a2d2986bb-0: prompt: 'Write a short story on Cristiano Ronaldo.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 3446, 389, 100215, 65515, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55688 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:51 engine.py:267] Added request cmpl-65adac63ab50426c927a5e8a2d2986bb-0.
INFO 01-23 06:49:52 logger.py:37] Received request cmpl-6945587c50bb482589905c59983c325f-0: prompt: 'Write a short description of the transformer architecture in neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 279, 43678, 18112, 304, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41340 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:52 engine.py:267] Added request cmpl-6945587c50bb482589905c59983c325f-0.
INFO 01-23 06:49:53 logger.py:37] Received request cmpl-bf95cb616d7e45ea8a79d06592658e88-0: prompt: 'Explain quantum computing in simple terms', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 31228, 25213, 304, 4382, 3878], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48096 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:53 engine.py:267] Added request cmpl-bf95cb616d7e45ea8a79d06592658e88-0.
INFO 01-23 06:49:55 logger.py:37] Received request cmpl-b0518f8a54234176a31a7683fe119a5e-0: prompt: 'Write a poem about artificial intelligence', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 33894, 922, 21075, 11478], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50058 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:55 engine.py:267] Added request cmpl-b0518f8a54234176a31a7683fe119a5e-0.
INFO 01-23 06:49:55 metrics.py:467] Avg prompt throughput: 8.1 tokens/s, Avg generation throughput: 237.4 tokens/s, Running: 36 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.2%, CPU KV cache usage: 0.0%.
INFO 01-23 06:49:55 logger.py:37] Received request cmpl-8a1987cb8c8e43ac899e0a87bc6416e9-0: prompt: 'Write a short description of recommendation systems using neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 28782, 6067, 1701, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46158 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:55 engine.py:267] Added request cmpl-8a1987cb8c8e43ac899e0a87bc6416e9-0.
INFO 01-23 06:49:57 logger.py:37] Received request cmpl-bdc2e0900e024d4fbdee6ce7716dbde0-0: prompt: 'Write a short description of Convolutional Neural Networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 30088, 3294, 278, 61577, 39810, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57386 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:58 engine.py:267] Added request cmpl-bdc2e0900e024d4fbdee6ce7716dbde0-0.
INFO 01-23 06:49:58 logger.py:37] Received request cmpl-b18b1262512144f78f619b7f1cad3d48-0: prompt: 'How does photosynthesis work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 4438, 1587, 7397, 74767, 990, 30], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47698 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:58 engine.py:267] Added request cmpl-b18b1262512144f78f619b7f1cad3d48-0.
INFO 01-23 06:49:59 logger.py:37] Received request cmpl-b773ff6484864b9aaee4d03d9356e176-0: prompt: 'Write a short description of artificial neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 21075, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45076 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:59 engine.py:267] Added request cmpl-b773ff6484864b9aaee4d03d9356e176-0.
INFO 01-23 06:49:59 logger.py:37] Received request cmpl-7069a22de2a141e9b9883d63dd0b5fd6-0: prompt: 'Write a short description of diffusion models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 58430, 4211, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56690 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:49:59 engine.py:267] Added request cmpl-7069a22de2a141e9b9883d63dd0b5fd6-0.
INFO 01-23 06:50:00 logger.py:37] Received request cmpl-90afa77df6f94dfa96774d0f81aab6ca-0: prompt: 'Write a short description of recommendation systems using neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 28782, 6067, 1701, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44214 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:00 engine.py:267] Added request cmpl-90afa77df6f94dfa96774d0f81aab6ca-0.
INFO 01-23 06:50:00 logger.py:37] Received request cmpl-a0f207f7a6e24ea89217a9585be0a4ba-0: prompt: 'Write a short description of the transformer architecture in neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 279, 43678, 18112, 304, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO 01-23 06:50:00 logger.py:37] Received request cmpl-727e2b7472854b1a94fae6dbb5a715ad-0: prompt: 'Explain double descent in neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 2033, 38052, 304, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34960 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:55476 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:00 engine.py:267] Added request cmpl-a0f207f7a6e24ea89217a9585be0a4ba-0.
INFO 01-23 06:50:00 engine.py:267] Added request cmpl-727e2b7472854b1a94fae6dbb5a715ad-0.
INFO 01-23 06:50:00 metrics.py:467] Avg prompt throughput: 13.4 tokens/s, Avg generation throughput: 243.5 tokens/s, Running: 36 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%.
INFO 01-23 06:50:02 logger.py:37] Received request cmpl-fe58733846f14a57a534894d997bd840-0: prompt: 'Explain double descent in neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 2033, 38052, 304, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55488 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:02 engine.py:267] Added request cmpl-fe58733846f14a57a534894d997bd840-0.
INFO 01-23 06:50:03 logger.py:37] Received request cmpl-16d475d8e7f5403f975c1f225edb1c2b-0: prompt: 'Explain double descent in neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 2033, 38052, 304, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55498 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:03 engine.py:267] Added request cmpl-16d475d8e7f5403f975c1f225edb1c2b-0.
INFO 01-23 06:50:03 logger.py:37] Received request cmpl-492df513832841c0a0de6b7e2fa93b78-0: prompt: 'Write a short story on Cristiano Ronaldo.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 3446, 389, 100215, 65515, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35452 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:03 engine.py:267] Added request cmpl-492df513832841c0a0de6b7e2fa93b78-0.
INFO 01-23 06:50:04 logger.py:37] Received request cmpl-d513ac179cfb4d5ca50e5ef30832ebb3-0: prompt: 'Write a short description of recommendation systems using neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 28782, 6067, 1701, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39008 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:04 engine.py:267] Added request cmpl-d513ac179cfb4d5ca50e5ef30832ebb3-0.
INFO 01-23 06:50:04 logger.py:37] Received request cmpl-7595ab6721b246c88f48652d437dce09-0: prompt: 'Explain double descent in neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 2033, 38052, 304, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55506 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:04 engine.py:267] Added request cmpl-7595ab6721b246c88f48652d437dce09-0.
INFO 01-23 06:50:05 logger.py:37] Received request cmpl-e7e13671f82b4fae97cf2b171a83fae1-0: prompt: 'Write a short description of recommendation systems using neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 28782, 6067, 1701, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44222 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:05 engine.py:267] Added request cmpl-e7e13671f82b4fae97cf2b171a83fae1-0.
INFO 01-23 06:50:05 metrics.py:467] Avg prompt throughput: 16.2 tokens/s, Avg generation throughput: 247.6 tokens/s, Running: 36 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%.
INFO 01-23 06:50:06 logger.py:37] Received request cmpl-b0697400c68447558b20767643bdc822-0: prompt: 'Explain double descent in neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 2033, 38052, 304, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57724 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:06 engine.py:267] Added request cmpl-b0697400c68447558b20767643bdc822-0.
INFO 01-23 06:50:06 logger.py:37] Received request cmpl-de1ca8386556490784d61b420aa51a64-0: prompt: 'Write a short description of diffusion models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 58430, 4211, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56696 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:06 engine.py:267] Added request cmpl-de1ca8386556490784d61b420aa51a64-0.
INFO 01-23 06:50:07 logger.py:37] Received request cmpl-15adaf72cbe444d983207e8fb8f4b9e9-0: prompt: 'Write a short description of Convolutional Neural Networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 30088, 3294, 278, 61577, 39810, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36058 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:07 engine.py:267] Added request cmpl-15adaf72cbe444d983207e8fb8f4b9e9-0.
INFO 01-23 06:50:08 logger.py:37] Received request cmpl-d9f04bcaaba047679aa528cc4cfe662e-0: prompt: 'Explain quantum computing in simple terms', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 31228, 25213, 304, 4382, 3878], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37208 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:08 engine.py:267] Added request cmpl-d9f04bcaaba047679aa528cc4cfe662e-0.
INFO 01-23 06:50:08 logger.py:37] Received request cmpl-efc1ce78c358499f9f3081a1e577b53a-0: prompt: 'Explain double descent in neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 2033, 38052, 304, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57736 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:08 engine.py:267] Added request cmpl-efc1ce78c358499f9f3081a1e577b53a-0.
INFO 01-23 06:50:08 logger.py:37] Received request cmpl-babee018c58c4582948ccef89211ac68-0: prompt: 'Write a poem about artificial intelligence', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 33894, 922, 21075, 11478], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54386 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:09 engine.py:267] Added request cmpl-babee018c58c4582948ccef89211ac68-0.
INFO 01-23 06:50:09 logger.py:37] Received request cmpl-23991e7904f44b8a9df2cef9c4b93912-0: prompt: 'Write a short description of the transformer architecture in neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 279, 43678, 18112, 304, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34964 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:09 engine.py:267] Added request cmpl-23991e7904f44b8a9df2cef9c4b93912-0.
INFO 01-23 06:50:10 logger.py:37] Received request cmpl-641bf8acc2914c3da65bc1ab92134a35-0: prompt: 'Write a short description of artificial neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 21075, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47802 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:10 engine.py:267] Added request cmpl-641bf8acc2914c3da65bc1ab92134a35-0.
INFO 01-23 06:50:10 logger.py:37] Received request cmpl-c15fb229a6e447dc832dd23787ffa43e-0: prompt: 'Write a short description of recommendation systems using neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 28782, 6067, 1701, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60722 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:10 engine.py:267] Added request cmpl-c15fb229a6e447dc832dd23787ffa43e-0.
INFO 01-23 06:50:10 metrics.py:467] Avg prompt throughput: 15.2 tokens/s, Avg generation throughput: 244.0 tokens/s, Running: 36 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.3%, CPU KV cache usage: 0.0%.
INFO 01-23 06:50:10 logger.py:37] Received request cmpl-38a178f435d54725b8b50792cb2ec5a4-0: prompt: 'Explain double descent in neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 2033, 38052, 304, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57748 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:10 engine.py:267] Added request cmpl-38a178f435d54725b8b50792cb2ec5a4-0.
INFO 01-23 06:50:11 logger.py:37] Received request cmpl-9851dc5a982d431b9b2b05cdeac0c956-0: prompt: 'How does photosynthesis work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 4438, 1587, 7397, 74767, 990, 30], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41556 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:11 engine.py:267] Added request cmpl-9851dc5a982d431b9b2b05cdeac0c956-0.
INFO 01-23 06:50:13 logger.py:37] Received request cmpl-2aa1dd18a49d40ce90368b06ab94ee27-0: prompt: 'Explain double descent in neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 2033, 38052, 304, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57752 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:13 engine.py:267] Added request cmpl-2aa1dd18a49d40ce90368b06ab94ee27-0.
INFO 01-23 06:50:14 logger.py:37] Received request cmpl-a38c561aa8024646a28ff24164183599-0: prompt: 'Write a short description of diffusion models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 58430, 4211, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:52506 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:14 engine.py:267] Added request cmpl-a38c561aa8024646a28ff24164183599-0.
INFO 01-23 06:50:15 logger.py:37] Received request cmpl-3440e4e96cba4928873acc9b00554bd9-0: prompt: 'Write a short story on Cristiano Ronaldo.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 3446, 389, 100215, 65515, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:59632 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:15 engine.py:267] Added request cmpl-3440e4e96cba4928873acc9b00554bd9-0.
INFO 01-23 06:50:15 metrics.py:467] Avg prompt throughput: 9.0 tokens/s, Avg generation throughput: 251.9 tokens/s, Running: 37 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%.
INFO 01-23 06:50:16 logger.py:37] Received request cmpl-fa8144baf56142bc9577943c312928e7-0: prompt: 'Write a short description of recommendation systems using neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 28782, 6067, 1701, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60724 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:16 engine.py:267] Added request cmpl-fa8144baf56142bc9577943c312928e7-0.
INFO 01-23 06:50:17 logger.py:37] Received request cmpl-239980ef8e884121947fb7c8096ef0c7-0: prompt: 'Explain double descent in neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 2033, 38052, 304, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46142 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:17 engine.py:267] Added request cmpl-239980ef8e884121947fb7c8096ef0c7-0.
INFO 01-23 06:50:17 logger.py:37] Received request cmpl-1447448c08c14481914f9d778b681c89-0: prompt: 'Write a short description of Convolutional Neural Networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 30088, 3294, 278, 61577, 39810, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48674 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:18 engine.py:267] Added request cmpl-1447448c08c14481914f9d778b681c89-0.
INFO 01-23 06:50:18 logger.py:37] Received request cmpl-9ef7f0c01d27456d95ab589ac0345318-0: prompt: 'Write a short description of the transformer architecture in neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 279, 43678, 18112, 304, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57586 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:18 engine.py:267] Added request cmpl-9ef7f0c01d27456d95ab589ac0345318-0.
INFO 01-23 06:50:20 logger.py:37] Received request cmpl-e0236c577d64480faf8f07ded74e4538-0: prompt: 'Explain double descent in neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 2033, 38052, 304, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46148 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:20 engine.py:267] Added request cmpl-e0236c577d64480faf8f07ded74e4538-0.
INFO 01-23 06:50:21 metrics.py:467] Avg prompt throughput: 10.7 tokens/s, Avg generation throughput: 251.4 tokens/s, Running: 37 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.0%, CPU KV cache usage: 0.0%.
INFO 01-23 06:50:21 logger.py:37] Received request cmpl-3ad0ea960bfa4d0185648b1476cf979b-0: prompt: 'Write a short description of artificial neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 21075, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55688 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:21 engine.py:267] Added request cmpl-3ad0ea960bfa4d0185648b1476cf979b-0.
INFO 01-23 06:50:21 logger.py:37] Received request cmpl-ca4a1ca8f20e49c78bdf898689c18e24-0: prompt: 'Write a short description of diffusion models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 58430, 4211, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41340 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:21 engine.py:267] Added request cmpl-ca4a1ca8f20e49c78bdf898689c18e24-0.
INFO 01-23 06:50:22 logger.py:37] Received request cmpl-988c27e10c5d4c23beb49f5d8ce5f6e8-0: prompt: 'Explain quantum computing in simple terms', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 31228, 25213, 304, 4382, 3878], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39238 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:22 engine.py:267] Added request cmpl-988c27e10c5d4c23beb49f5d8ce5f6e8-0.
INFO 01-23 06:50:23 logger.py:37] Received request cmpl-456788570afc4b0291587765eada74d4-0: prompt: 'Write a poem about artificial intelligence', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 33894, 922, 21075, 11478], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:48096 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:23 engine.py:267] Added request cmpl-456788570afc4b0291587765eada74d4-0.
INFO 01-23 06:50:24 logger.py:37] Received request cmpl-bd3800092d56431ba806f3d3089a73da-0: prompt: 'How does photosynthesis work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 4438, 1587, 7397, 74767, 990, 30], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:50058 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:24 engine.py:267] Added request cmpl-bd3800092d56431ba806f3d3089a73da-0.
INFO 01-23 06:50:25 logger.py:37] Received request cmpl-0bce833a74d6487d900173c87d6b534d-0: prompt: 'Explain double descent in neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 2033, 38052, 304, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:46158 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:25 engine.py:267] Added request cmpl-0bce833a74d6487d900173c87d6b534d-0.
INFO 01-23 06:50:26 metrics.py:467] Avg prompt throughput: 11.6 tokens/s, Avg generation throughput: 258.7 tokens/s, Running: 38 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.3%, CPU KV cache usage: 0.0%.
INFO 01-23 06:50:27 logger.py:37] Received request cmpl-5e27daa10a7f4ffcbf17db525bd8afc0-0: prompt: 'Write a short description of the transformer architecture in neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 279, 43678, 18112, 304, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57386 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:27 engine.py:267] Added request cmpl-5e27daa10a7f4ffcbf17db525bd8afc0-0.
INFO 01-23 06:50:28 logger.py:37] Received request cmpl-aca8a336f9594c41b993e940acdb4242-0: prompt: 'Write a short story on Cristiano Ronaldo.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 3446, 389, 100215, 65515, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47698 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:28 engine.py:267] Added request cmpl-aca8a336f9594c41b993e940acdb4242-0.
INFO 01-23 06:50:28 logger.py:37] Received request cmpl-da3c6539136b46f38e03abacf0ecda4e-0: prompt: 'Write a short description of Convolutional Neural Networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 30088, 3294, 278, 61577, 39810, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:45076 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:28 engine.py:267] Added request cmpl-da3c6539136b46f38e03abacf0ecda4e-0.
INFO 01-23 06:50:29 logger.py:37] Received request cmpl-a922b5eb250046ffa406c8dd6142a5ec-0: prompt: 'Write a short description of recommendation systems using neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 28782, 6067, 1701, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56690 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:29 engine.py:267] Added request cmpl-a922b5eb250046ffa406c8dd6142a5ec-0.
INFO 01-23 06:50:29 logger.py:37] Received request cmpl-706dae2d23884fad96ca659a6d9f921f-0: prompt: 'Explain double descent in neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 2033, 38052, 304, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44214 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:29 engine.py:267] Added request cmpl-706dae2d23884fad96ca659a6d9f921f-0.
INFO 01-23 06:50:29 logger.py:37] Received request cmpl-81c3d743b9a144e6a601a71693c83848-0: prompt: 'Write a short description of diffusion models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 58430, 4211, 13], lora_request: None, prompt_adapter_request: None.
INFO 01-23 06:50:29 logger.py:37] Received request cmpl-be1b001bd5044b19b1b6a6dc3c307809-0: prompt: 'Explain quantum computing in simple terms', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 31228, 25213, 304, 4382, 3878], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34960 - "POST /v1/completions HTTP/1.1" 200 OK
INFO:     127.0.0.1:55476 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:30 engine.py:267] Added request cmpl-81c3d743b9a144e6a601a71693c83848-0.
INFO 01-23 06:50:30 engine.py:267] Added request cmpl-be1b001bd5044b19b1b6a6dc3c307809-0.
INFO 01-23 06:50:31 metrics.py:467] Avg prompt throughput: 14.4 tokens/s, Avg generation throughput: 256.9 tokens/s, Running: 38 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 4.2%, CPU KV cache usage: 0.0%.
INFO 01-23 06:50:31 logger.py:37] Received request cmpl-5a0037179c4e4c78b5677a889052a420-0: prompt: 'Explain quantum computing in simple terms', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 31228, 25213, 304, 4382, 3878], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55488 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:31 engine.py:267] Added request cmpl-5a0037179c4e4c78b5677a889052a420-0.
INFO 01-23 06:50:32 logger.py:37] Received request cmpl-39c34fe2492d42c789091fa7a712eb72-0: prompt: 'Explain quantum computing in simple terms', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 31228, 25213, 304, 4382, 3878], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55498 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:32 engine.py:267] Added request cmpl-39c34fe2492d42c789091fa7a712eb72-0.
INFO 01-23 06:50:33 logger.py:37] Received request cmpl-d8973efc9fe6495e811514232ac319f7-0: prompt: 'Write a short description of artificial neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 21075, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:35452 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:33 engine.py:267] Added request cmpl-d8973efc9fe6495e811514232ac319f7-0.
INFO 01-23 06:50:33 logger.py:37] Received request cmpl-cbb85152ba244832854396fa6b2a9969-0: prompt: 'Explain double descent in neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 2033, 38052, 304, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:39008 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:33 engine.py:267] Added request cmpl-cbb85152ba244832854396fa6b2a9969-0.
INFO 01-23 06:50:34 logger.py:37] Received request cmpl-b50b7b0ca56a4a208779ccd579eeb232-0: prompt: 'Explain quantum computing in simple terms', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 31228, 25213, 304, 4382, 3878], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:55506 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:34 engine.py:267] Added request cmpl-b50b7b0ca56a4a208779ccd579eeb232-0.
INFO 01-23 06:50:34 logger.py:37] Received request cmpl-0f496affc07c42deb81662509dd3f91d-0: prompt: 'Explain double descent in neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 2033, 38052, 304, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:44222 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:34 engine.py:267] Added request cmpl-0f496affc07c42deb81662509dd3f91d-0.
INFO 01-23 06:50:35 logger.py:37] Received request cmpl-340c90f5a025441fb1eac6961483afdc-0: prompt: 'Explain quantum computing in simple terms', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 31228, 25213, 304, 4382, 3878], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57724 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:35 engine.py:267] Added request cmpl-340c90f5a025441fb1eac6961483afdc-0.
INFO 01-23 06:50:36 metrics.py:467] Avg prompt throughput: 11.9 tokens/s, Avg generation throughput: 255.7 tokens/s, Running: 37 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%.
INFO 01-23 06:50:36 logger.py:37] Received request cmpl-3d44e0939c2b4047ac3a52d82259d9d5-0: prompt: 'Write a short description of recommendation systems using neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 28782, 6067, 1701, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:56696 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:36 engine.py:267] Added request cmpl-3d44e0939c2b4047ac3a52d82259d9d5-0.
INFO 01-23 06:50:37 logger.py:37] Received request cmpl-38d0907450fc4e96af9b4b5fb16eb7cd-0: prompt: 'Write a short description of the transformer architecture in neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 279, 43678, 18112, 304, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:36058 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:37 engine.py:267] Added request cmpl-38d0907450fc4e96af9b4b5fb16eb7cd-0.
INFO 01-23 06:50:37 logger.py:37] Received request cmpl-8ef6b1a978fc4cb1a77bab842d7aab62-0: prompt: 'Write a poem about artificial intelligence', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 33894, 922, 21075, 11478], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:37208 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:37 engine.py:267] Added request cmpl-8ef6b1a978fc4cb1a77bab842d7aab62-0.
INFO 01-23 06:50:37 logger.py:37] Received request cmpl-c9771ddd4f40494d9b696bdf89b9e507-0: prompt: 'Explain quantum computing in simple terms', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 31228, 25213, 304, 4382, 3878], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57736 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:37 engine.py:267] Added request cmpl-c9771ddd4f40494d9b696bdf89b9e507-0.
INFO 01-23 06:50:38 logger.py:37] Received request cmpl-10fed5cc544f485891d7065c6fa6e95d-0: prompt: 'Explain quantum computing in simple terms', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 31228, 25213, 304, 4382, 3878], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47546 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:38 engine.py:267] Added request cmpl-10fed5cc544f485891d7065c6fa6e95d-0.
INFO 01-23 06:50:38 logger.py:37] Received request cmpl-94d0075a81764238b1e1130756a9e13a-0: prompt: 'How does photosynthesis work?', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 4438, 1587, 7397, 74767, 990, 30], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:54386 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:38 engine.py:267] Added request cmpl-94d0075a81764238b1e1130756a9e13a-0.
INFO 01-23 06:50:38 logger.py:37] Received request cmpl-a68a6ac7b26a40e3b78176a15a8a70fb-0: prompt: 'Write a short description of diffusion models.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 58430, 4211, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:34964 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:38 engine.py:267] Added request cmpl-a68a6ac7b26a40e3b78176a15a8a70fb-0.
INFO 01-23 06:50:39 logger.py:37] Received request cmpl-850dcdbe41144fb98062c68a310855e9-0: prompt: 'Write a short description of Convolutional Neural Networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 4096, 315, 30088, 3294, 278, 61577, 39810, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:47802 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:39 engine.py:267] Added request cmpl-850dcdbe41144fb98062c68a310855e9-0.
INFO 01-23 06:50:40 logger.py:37] Received request cmpl-2dfddc00e4b6454faacb5a655bc4814d-0: prompt: 'Explain double descent in neural networks.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 2033, 38052, 304, 30828, 14488, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:60722 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:40 engine.py:267] Added request cmpl-2dfddc00e4b6454faacb5a655bc4814d-0.
INFO 01-23 06:50:40 logger.py:37] Received request cmpl-aafd57c7f2e24721b552eac26fa407d2-0: prompt: 'Explain quantum computing in simple terms', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 849, 21435, 31228, 25213, 304, 4382, 3878], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:57748 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:40 engine.py:267] Added request cmpl-aafd57c7f2e24721b552eac26fa407d2-0.
INFO 01-23 06:50:41 metrics.py:467] Avg prompt throughput: 18.6 tokens/s, Avg generation throughput: 260.7 tokens/s, Running: 38 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.5%, CPU KV cache usage: 0.0%.
INFO 01-23 06:50:41 logger.py:37] Received request cmpl-fc54cfd0b50a474d92a399170a5eaa3e-0: prompt: 'Write a short story on Cristiano Ronaldo.', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=200, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None), prompt_token_ids: [128000, 8144, 264, 2875, 3446, 389, 100215, 65515, 13], lora_request: None, prompt_adapter_request: None.
INFO:     127.0.0.1:41556 - "POST /v1/completions HTTP/1.1" 200 OK
INFO 01-23 06:50:41 engine.py:267] Added request cmpl-fc54cfd0b50a474d92a399170a5eaa3e-0.
INFO 01-23 06:50:46 metrics.py:467] Avg prompt throughput: 1.8 tokens/s, Avg generation throughput: 257.1 tokens/s, Running: 35 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.9%, CPU KV cache usage: 0.0%.
INFO 01-23 06:50:51 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 221.2 tokens/s, Running: 29 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.7%, CPU KV cache usage: 0.0%.
INFO 01-23 06:50:56 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 183.5 tokens/s, Running: 25 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 3.8%, CPU KV cache usage: 0.0%.
INFO 01-23 06:51:01 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 143.0 tokens/s, Running: 17 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.9%, CPU KV cache usage: 0.0%.
INFO 01-23 06:51:06 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 89.9 tokens/s, Running: 8 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.5%, CPU KV cache usage: 0.0%.
INFO 01-23 06:51:19 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.4 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
INFO 01-23 06:51:29 metrics.py:467] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.
